{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Transaction Fraud Detection: Model Evaluation\n",
    "\n",
    "This notebook focuses on evaluating the Graph Neural Network (GNN) models we trained in the previous notebook. We'll conduct a comprehensive evaluation on the test set, including metrics such as accuracy, precision, recall, F1-score, ROC curves, and confusion matrices.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Loading Data and Models](#Loading-Data-and-Models)\n",
    "3. [Evaluation Metrics](#Evaluation-Metrics)\n",
    "4. [Model Evaluation](#Model-Evaluation)\n",
    "5. [Performance Analysis](#Performance-Analysis)\n",
    "   - [Confusion Matrix](#Confusion-Matrix)\n",
    "   - [ROC Curve](#ROC-Curve)\n",
    "   - [Precision-Recall Curve](#Precision-Recall-Curve)\n",
    "   - [Class-specific Performance](#Class-specific-Performance)\n",
    "6. [Error Analysis](#Error-Analysis)\n",
    "7. [Model Comparison](#Model-Comparison)\n",
    "8. [Generate Evaluation Report](#Generate-Evaluation-Report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_curve, auc, \n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, f1_score, roc_auc_score\n",
    ")\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Import the GNN models\n",
    "import sys\n",
    "sys.path.append('.') # Add current directory to path to import from notebooks\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "os.makedirs('reports/figures', exist_ok=True)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Models\n",
    "\n",
    "First, let's load the processed data and the trained GNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model classes to load the models\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(GCNModel, self).__init__()\n",
    "        # This is just a skeleton for loading the model weights\n",
    "        # The actual implementation is in the training notebook\n",
    "        pass\n",
    "\n",
    "class SAGEModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(SAGEModel, self).__init__()\n",
    "        # This is just a skeleton for loading the model weights\n",
    "        pass\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, heads=8, dropout=0.5):\n",
    "        super(GATModel, self).__init__()\n",
    "        # This is just a skeleton for loading the model weights\n",
    "        pass\n",
    "\n",
    "def load_processed_data(input_dir='data/processed'):\n",
    "    \"\"\"\n",
    "    Load processed data from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory with processed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data : torch_geometric.data.Data\n",
    "        PyTorch Geometric Data object\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading processed data from {input_dir}\")\n",
    "    \n",
    "    # Load data object\n",
    "    data_path = os.path.join(input_dir, 'data.pt')\n",
    "    \n",
    "    try:\n",
    "        # Try direct loading first\n",
    "        data = torch.load(data_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load data directly: {e}. Reconstructing from components...\")\n",
    "        # If that fails, try to reconstruct the data object from components\n",
    "        features_path = os.path.join(input_dir, 'features.npy')\n",
    "        labels_path = os.path.join(input_dir, 'labels.npy')\n",
    "        edge_index_path = os.path.join(input_dir, 'edge_index.pt')\n",
    "        \n",
    "        # Check if we have combined features (from feature engineering)\n",
    "        combined_features_path = os.path.join(input_dir, 'combined_features.npy')\n",
    "        if os.path.exists(combined_features_path):\n",
    "            logger.info(\"Loading combined features from feature engineering\")\n",
    "            features = torch.FloatTensor(np.load(combined_features_path))\n",
    "        else:\n",
    "            features = torch.FloatTensor(np.load(features_path))\n",
    "            \n",
    "        labels = torch.LongTensor(np.load(labels_path))\n",
    "        edge_index = torch.load(edge_index_path)\n",
    "        \n",
    "        from torch_geometric.data import Data\n",
    "        data = Data(x=features, edge_index=edge_index, y=labels)\n",
    "    \n",
    "    # Load splits\n",
    "    split_idx = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(input_dir, f'{split}_idx.npy')\n",
    "        if os.path.exists(split_path):\n",
    "            split_idx[split] = np.load(split_path)\n",
    "            logger.info(f\"Loaded {split} indices with {len(split_idx[split])} samples\")\n",
    "    \n",
    "    logger.info(f\"Successfully loaded processed data from {input_dir}\")\n",
    "    logger.info(f\"Data contains {data.num_nodes} nodes, {data.num_edges} edges, and {data.num_features} features\")\n",
    "    \n",
    "    return data, split_idx\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    data, split_idx = load_processed_data()\n",
    "    \n",
    "    # Print information about the data\n",
    "    print(\"Data information:\")\n",
    "    print(f\"Number of nodes: {data.num_nodes}\")\n",
    "    print(f\"Number of edges: {data.num_edges}\")\n",
    "    print(f\"Number of features: {data.num_features}\")\n",
    "    print(f\"Number of classes: {len(torch.unique(data.y))}\")\n",
    "    \n",
    "    # Print split sizes\n",
    "    print(\"\\nSplit sizes:\")\n",
    "    for split_name, indices in split_idx.items():\n",
    "        print(f\"{split_name.capitalize()}: {len(indices)} nodes\")\n",
    "        \n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Load information about the best model\n",
    "    best_model_name_path = os.path.join('models', 'best_model_name.txt')\n",
    "    if os.path.exists(best_model_name_path):\n",
    "        with open(best_model_name_path, 'r') as f:\n",
    "            best_model_name = f.read().strip()\n",
    "        print(f\"\\nBest model: {best_model_name}\")\n",
    "    else:\n",
    "        logger.warning(\"Best model name not found. Will evaluate individual models if available.\")\n",
    "        best_model_name = None\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error loading data: {e}. Please make sure you've run the previous notebooks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Let's define functions to evaluate our models and compute various performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, split_idx, criterion=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The trained model\n",
    "    data : torch_geometric.data.Data\n",
    "        The graph data\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    criterion : torch.nn.Module, optional\n",
    "        Loss function to calculate loss\n",
    "    device : str\n",
    "        Device to use ('cpu' or 'cuda')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    raw_data : dict\n",
    "        Dictionary containing raw predictions\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Calculate loss if criterion is provided\n",
    "        loss = {}\n",
    "        if criterion is not None:\n",
    "            for split in split_idx:\n",
    "                loss[split] = criterion(out[split_idx[split]], data.y[split_idx[split]]).item()\n",
    "        \n",
    "        # Get predictions and probabilities\n",
    "        preds = {}\n",
    "        probs = {}\n",
    "        \n",
    "        # Get raw probabilities\n",
    "        raw_probs = torch.exp(out)\n",
    "        \n",
    "        # Number of classes\n",
    "        num_classes = raw_probs.shape[1]\n",
    "        \n",
    "        for split in split_idx:\n",
    "            preds[split] = out.argmax(dim=1)[split_idx[split]].cpu().numpy()\n",
    "            \n",
    "            # For binary classification, use probability of class 1\n",
    "            # For multi-class, use all probabilities\n",
    "            if num_classes == 2:\n",
    "                probs[split] = raw_probs[split_idx[split], 1].cpu().numpy()\n",
    "            else:\n",
    "                probs[split] = raw_probs[split_idx[split]].cpu().numpy()\n",
    "    \n",
    "    # Collect true labels\n",
    "    y_true = {}\n",
    "    for split in split_idx:\n",
    "        y_true[split] = data.y[split_idx[split]].cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {split: {} for split in split_idx}\n",
    "    \n",
    "    for split in split_idx:\n",
    "        # Add loss if available\n",
    "        if criterion is not None and split in loss:\n",
    "            metrics[split]['loss'] = loss[split]\n",
    "        \n",
    "        # Classification report\n",
    "        try:\n",
    "            report = classification_report(y_true[split], preds[split], output_dict=True)\n",
    "            \n",
    "            # Add metrics from report\n",
    "            for k, v in report.items():\n",
    "                if isinstance(v, dict):  # Class-specific metrics\n",
    "                    for metric, value in v.items():\n",
    "                        metrics[split][f\"{k}_{metric}\"] = value\n",
    "                else:  # Overall metrics like accuracy\n",
    "                    metrics[split][k] = v\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error generating classification report for {split}: {str(e)}\")\n",
    "            metrics[split]['accuracy'] = (y_true[split] == preds[split]).mean()\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        metrics[split]['confusion_matrix'] = confusion_matrix(y_true[split], preds[split]).tolist()\n",
    "        \n",
    "        # Calculate macro-average metrics if there are multiple classes\n",
    "        unique_classes = np.unique(y_true[split])\n",
    "        if len(unique_classes) > 1:\n",
    "            # ROC AUC (one-vs-rest for multi-class)\n",
    "            try:\n",
    "                if num_classes == 2:\n",
    "                    metrics[split]['roc_auc'] = roc_auc_score(y_true[split], probs[split])\n",
    "                else:\n",
    "                    # For multi-class, compute one-vs-rest AUC for each class\n",
    "                    aucs = []\n",
    "                    for i in range(num_classes):\n",
    "                        if i in unique_classes:\n",
    "                            y_true_bin = (y_true[split] == i).astype(int)\n",
    "                            if isinstance(probs[split], np.ndarray) and probs[split].ndim == 2:\n",
    "                                class_probs = probs[split][:, i]\n",
    "                                aucs.append(roc_auc_score(y_true_bin, class_probs))\n",
    "                    if aucs:\n",
    "                        metrics[split]['roc_auc'] = np.mean(aucs)\n",
    "                    else:\n",
    "                        metrics[split]['roc_auc'] = float('nan')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error calculating ROC AUC for {split}: {str(e)}\")\n",
    "                metrics[split]['roc_auc'] = float('nan')\n",
    "                \n",
    "            # PR AUC (one-vs-rest for multi-class)\n",
    "            try:\n",
    "                if num_classes == 2:\n",
    "                    metrics[split]['pr_auc'] = average_precision_score(y_true[split], probs[split])\n",
    "                else:\n",
    "                    # For multi-class, compute one-vs-rest PR AUC for each class\n",
    "                    pr_aucs = []\n",
    "                    for i in range(num_classes):\n",
    "                        if i in unique_classes:\n",
    "                            y_true_bin = (y_true[split] == i).astype(int)\n",
    "                            if isinstance(probs[split], np.ndarray) and probs[split].ndim == 2:\n",
    "                                class_probs = probs[split][:, i]\n",
    "                                pr_aucs.append(average_precision_score(y_true_bin, class_probs))\n",
    "                    if pr_aucs:\n",
    "                        metrics[split]['pr_auc'] = np.mean(pr_aucs)\n",
    "                    else:\n",
    "                        metrics[split]['pr_auc'] = float('nan')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error calculating PR AUC for {split}: {str(e)}\")\n",
    "                metrics[split]['pr_auc'] = float('nan')\n",
    "    \n",
    "    # Store raw predictions for further analysis\n",
    "    raw_data = {\n",
    "        split: {\n",
    "            'y_true': y_true[split],\n",
    "            'y_pred': preds[split],\n",
    "            'probabilities': probs[split]\n",
    "        } for split in split_idx\n",
    "    }\n",
    "    \n",
    "    return metrics, raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, output_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        True binary labels\n",
    "    y_score : numpy.ndarray\n",
    "        Target scores (probabilities)\n",
    "    output_path : str, optional\n",
    "        Path to save the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The generated figure\n",
    "    \"\"\"\n",
    "    # Check if it's binary classification\n",
    "    if len(np.unique(y_true)) != 2:\n",
    "        logger.warning(\"ROC curve requires binary classification. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "            label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_score, output_path=None):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curve.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        True binary labels\n",
    "    y_score : numpy.ndarray\n",
    "        Target scores (probabilities)\n",
    "    output_path : str, optional\n",
    "        Path to save the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The generated figure\n",
    "    \"\"\"\n",
    "    # Check if it's binary classification\n",
    "    if len(np.unique(y_true)) != 2:\n",
    "        logger.warning(\"Precision-Recall curve requires binary classification. Skipping.\")\n",
    "        return None\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    avg_precision = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.step(recall, precision, color='darkorange', lw=2, where='post',\n",
    "            label=f'AP = {avg_precision:.3f}')\n",
    "    ax.fill_between(recall, precision, step='post', alpha=0.2, color='darkorange')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall Curve')\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, output_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy.ndarray\n",
    "        True labels\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted labels\n",
    "    output_path : str, optional\n",
    "        Path to save the plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The generated figure\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    \n",
    "    # Set x and y tick labels\n",
    "    classes = sorted(np.unique(np.concatenate((y_true, y_pred))))\n",
    "    class_labels = ['Legitimate' if c==0 else 'Fraudulent' for c in classes]\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report(metrics, raw_data, model_name, output_dir='reports'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive evaluation report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics : dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    raw_data : dict\n",
    "        Dictionary containing raw predictions\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    output_dir : str\n",
    "        Directory to save report files\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    report_path : str\n",
    "        Path to the generated report\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create report subdirectory\n",
    "    report_dir = os.path.join(output_dir, model_name)\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    # Create figures directory\n",
    "    figures_dir = os.path.join(report_dir, 'figures')\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate plots for binary classification\n",
    "    for split in raw_data:\n",
    "        y_true = raw_data[split]['y_true']\n",
    "        y_pred = raw_data[split]['y_pred']\n",
    "        probs = raw_data[split]['probabilities']\n",
    "        \n",
    "        # Confusion matrix (works for any number of classes)\n",
    "        plot_confusion_matrix(y_true, y_pred, \n",
    "                             output_path=os.path.join(figures_dir, f'{split}_confusion_matrix.png'))\n",
    "        \n",
    "        # ROC and PR curves (only for binary classification)\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            # ROC curve\n",
    "            plot_roc_curve(y_true, probs, \n",
    "                          output_path=os.path.join(figures_dir, f'{split}_roc_curve.png'))\n",
    "            \n",
    "            # Precision-Recall curve\n",
    "            plot_precision_recall_curve(y_true, probs, \n",
    "                                       output_path=os.path.join(figures_dir, f'{split}_pr_curve.png'))\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    metrics_path = os.path.join(report_dir, 'metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Create markdown report\n",
    "    report_md = f\"# Evaluation Report for {model_name}\\n\\n\"\n",
    "    \n",
    "    # Add summary section\n",
    "    report_md += \"## Summary\\n\\n\"\n",
    "    report_md += \"| Metric | Train | Validation | Test |\\n\"\n",
    "    report_md += \"|--------|-------|------------|------|\\n\"\n",
    "    \n",
    "    # Key metrics to include in summary\n",
    "    key_metrics = ['accuracy', 'weighted avg_f1-score', 'weighted avg_precision', 'weighted avg_recall']\n",
    "    \n",
    "    # Add ROC AUC and PR AUC if available (binary classification)\n",
    "    if 'roc_auc' in metrics.get('test', {}):\n",
    "        key_metrics.extend(['roc_auc', 'pr_auc'])\n",
    "    \n",
    "    for metric in key_metrics:\n",
    "        train_val = metrics.get('train', {}).get(metric, 'N/A')\n",
    "        val_val = metrics.get('val', {}).get(metric, 'N/A')\n",
    "        test_val = metrics.get('test', {}).get(metric, 'N/A')\n",
    "        \n",
    "        # Format values\n",
    "        if isinstance(train_val, float):\n",
    "            train_val = f\"{train_val:.4f}\"\n",
    "        if isinstance(val_val, float):\n",
    "            val_val = f\"{val_val:.4f}\"\n",
    "        if isinstance(test_val, float):\n",
    "            test_val = f\"{test_val:.4f}\"\n",
    "        \n",
    "        report_md += f\"| {metric} | {train_val} | {val_val} | {test_val} |\\n\"\n",
    "    \n",
    "    # Add class-specific metrics if available\n",
    "    classes = set()\n",
    "    for split in metrics:\n",
    "        for k in metrics[split]:\n",
    "            if '_' in k and k.split('_')[0].isdigit():\n",
    "                classes.add(int(k.split('_')[0]))\n",
    "    \n",
    "    if classes:\n",
    "        report_md += \"\\n## Class-specific Metrics (Test Set)\\n\\n\"\n",
    "        report_md += \"| Class | Precision | Recall | F1-Score | Support |\\n\"\n",
    "        report_md += \"|-------|-----------|--------|----------|--------|\\n\"\n",
    "        \n",
    "        for cls in sorted(classes):\n",
    "            precision = metrics.get('test', {}).get(f\"{cls}_precision\", 'N/A')\n",
    "            recall = metrics.get('test', {}).get(f\"{cls}_recall\", 'N/A')\n",
    "            f1 = metrics.get('test', {}).get(f\"{cls}_f1-score\", 'N/A')\n",
    "            support = metrics.get('test', {}).get(f\"{cls}_support\", 'N/A')\n",
    "            \n",
    "            # Format values\n",
    "            if isinstance(precision, float):\n",
    "                precision = f\"{precision:.4f}\"\n",
    "            if isinstance(recall, float):\n",
    "                recall = f\"{recall:.4f}\"\n",
    "            if isinstance(f1, float):\n",
    "                f1 = f\"{f1:.4f}\"\n",
    "            \n",
    "            report_md += f\"| {cls} | {precision} | {recall} | {f1} | {support} |\\n\"\n",
    "    \n",
    "    # Add detailed metrics section\n",
    "    report_md += \"\\n## Detailed Metrics\\n\\n\"\n",
    "    \n",
    "    for split in metrics:\n",
    "        report_md += f\"### {split.capitalize()} Set\\n\\n\"\n",
    "        \n",
    "        # Add confusion matrix\n",
    "        report_md += \"#### Confusion Matrix\\n\\n\"\n",
    "        report_md += f\"![Confusion Matrix](figures/{split}_confusion_matrix.png)\\n\\n\"\n",
    "        \n",
    "        # Add ROC curve and PR curve if available (binary classification)\n",
    "        if 'roc_auc' in metrics[split]:\n",
    "            split_metrics = metrics[split]\n",
    "            unique_classes = set()\n",
    "            for k in split_metrics:\n",
    "                if '_' in k and k.split('_')[0].isdigit():\n",
    "                    unique_classes.add(int(k.split('_')[0]))\n",
    "            \n",
    "            if len(unique_classes) == 2:\n",
    "                # Add ROC curve\n",
    "                report_md += \"#### ROC Curve\\n\\n\"\n",
    "                report_md += f\"![ROC Curve](figures/{split}_roc_curve.png)\\n\\n\"\n",
    "                \n",
    "                # Add PR curve\n",
    "                report_md += \"#### Precision-Recall Curve\\n\\n\"\n",
    "                report_md += f\"![PR Curve](figures/{split}_pr_curve.png)\\n\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(report_dir, 'evaluation_report.md')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_md)\n",
    "    \n",
    "    logger.info(f\"Evaluation report generated at {report_path}\")\n",
    "    \n",
    "    return report_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now, let's evaluate the trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate_model(model_name, data, split_idx, device):\n",
    "    \"\"\"\n",
    "    Load and evaluate a model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model to load\n",
    "    data : torch_geometric.data.Data\n",
    "        The graph data\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    device : str\n",
    "        Device to use ('cpu' or 'cuda')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    raw_data : dict\n",
    "        Dictionary containing raw predictions\n",
    "    \"\"\"\n",
    "    # Path to model weights\n",
    "    model_path = os.path.join('models', f'{model_name}_best.pt')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        logger.error(f\"Model weights not found at {model_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create appropriate model\n",
    "    input_dim = data.x.shape[1]\n",
    "    hidden_dim = 256  # Same as in training\n",
    "    output_dim = len(torch.unique(data.y))\n",
    "    \n",
    "    if model_name.lower() == 'gcn':\n",
    "        from torch_geometric.nn import GCNConv\n",
    "        \n",
    "        class GCNModel(torch.nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "                super(GCNModel, self).__init__()\n",
    "                self.convs = torch.nn.ModuleList()\n",
    "                self.bns = torch.nn.ModuleList()\n",
    "                self.num_layers = num_layers\n",
    "                self.dropout = dropout\n",
    "                \n",
    "                # Input layer\n",
    "                self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "                \n",
    "                # Hidden layers\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "                    self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "                \n",
    "                # Output layer\n",
    "                self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "            \n",
    "            def forward(self, x, edge_index):\n",
    "                # Input layer\n",
    "                h = self.convs[0](x, edge_index)\n",
    "                h = torch.relu(h)\n",
    "                h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Hidden layers\n",
    "                for i in range(1, self.num_layers - 1):\n",
    "                    h_prev = h\n",
    "                    h = self.convs[i](h, edge_index)\n",
    "                    h = self.bns[i-1](h)\n",
    "                    h = torch.relu(h)\n",
    "                    h = h + h_prev  # Residual connection\n",
    "                    h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Output layer\n",
    "                h = self.convs[-1](h, edge_index)\n",
    "                \n",
    "                return torch.nn.functional.log_softmax(h, dim=1)\n",
    "                \n",
    "        model = GCNModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        \n",
    "    elif model_name.lower() == 'sage':\n",
    "        from torch_geometric.nn import SAGEConv\n",
    "        \n",
    "        class SAGEModel(torch.nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "                super(SAGEModel, self).__init__()\n",
    "                self.convs = torch.nn.ModuleList()\n",
    "                self.bns = torch.nn.ModuleList()\n",
    "                self.num_layers = num_layers\n",
    "                self.dropout = dropout\n",
    "                \n",
    "                # Input layer\n",
    "                self.convs.append(SAGEConv(input_dim, hidden_dim))\n",
    "                \n",
    "                # Hidden layers\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "                    self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "                \n",
    "                # Output layer\n",
    "                self.convs.append(SAGEConv(hidden_dim, output_dim))\n",
    "            \n",
    "            def forward(self, x, edge_index):\n",
    "                # Input layer\n",
    "                h = self.convs[0](x, edge_index)\n",
    "                h = torch.relu(h)\n",
    "                h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Hidden layers\n",
    "                for i in range(1, self.num_layers - 1):\n",
    "                    h_prev = h\n",
    "                    h = self.convs[i](h, edge_index)\n",
    "                    h = self.bns[i-1](h)\n",
    "                    h = torch.relu(h)\n",
    "                    h = h + h_prev  # Residual connection\n",
    "                    h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Output layer\n",
    "                h = self.convs[-1](h, edge_index)\n",
    "                \n",
    "                return torch.nn.functional.log_softmax(h, dim=1)\n",
    "                \n",
    "        model = SAGEModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "        \n",
    "    elif model_name.lower() == 'gat':\n",
    "        from torch_geometric.nn import GATConv\n",
    "        \n",
    "        class GATModel(torch.nn.Module):\n",
    "            def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, heads=8, dropout=0.5):\n",
    "                super(GATModel, self).__init__()\n",
    "                self.convs = torch.nn.ModuleList()\n",
    "                self.bns = torch.nn.ModuleList()\n",
    "                self.num_layers = num_layers\n",
    "                self.dropout = dropout\n",
    "                \n",
    "                # Input layer\n",
    "                self.convs.append(GATConv(input_dim, hidden_dim // heads, heads=heads))\n",
    "                \n",
    "                # Hidden layers\n",
    "                for _ in range(num_layers - 2):\n",
    "                    self.convs.append(GATConv(hidden_dim, hidden_dim // heads, heads=heads))\n",
    "                    self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "                \n",
    "                # Output layer\n",
    "                self.convs.append(GATConv(hidden_dim, output_dim, heads=1))\n",
    "            \n",
    "            def forward(self, x, edge_index):\n",
    "                # Input layer\n",
    "                h = self.convs[0](x, edge_index)\n",
    "                h = torch.relu(h)\n",
    "                h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Hidden layers\n",
    "                for i in range(1, self.num_layers - 1):\n",
    "                    h_prev = h\n",
    "                    h = self.convs[i](h, edge_index)\n",
    "                    h = self.bns[i-1](h)\n",
    "                    h = torch.relu(h)\n",
    "                    h = h + h_prev  # Residual connection\n",
    "                    h = torch.nn.functional.dropout(h, p=self.dropout, training=self.training)\n",
    "                \n",
    "                # Output layer\n",
    "                h = self.convs[-1](h, edge_index)\n",
    "                \n",
    "                return torch.nn.functional.log_softmax(h, dim=1)\n",
    "                \n",
    "        model = GATModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim).to(device)\n",
    "    else:\n",
    "        logger.error(f\"Unknown model type: {model_name}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load model weights\n",
    "        logger.info(f\"Loading model weights from {model_path}\")\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define loss function\n",
    "        criterion = torch.nn.NLLLoss()\n",
    "        \n",
    "        # Evaluate model\n",
    "        logger.info(f\"Evaluating {model_name} model\")\n",
    "        metrics, raw_data = evaluate_model(\n",
    "            model=model,\n",
    "            data=data,\n",
    "            split_idx=split_idx,\n",
    "            criterion=criterion,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        return metrics, raw_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error evaluating model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Evaluate the best model\n",
    "if best_model_name is not None:\n",
    "    logger.info(f\"Evaluating best model: {best_model_name}\")\n",
    "    metrics, raw_data = load_and_evaluate_model(best_model_name, data, split_idx, device)\n",
    "    \n",
    "    if metrics is not None:\n",
    "        # Generate evaluation report\n",
    "        report_path = generate_evaluation_report(metrics, raw_data, best_model_name)\n",
    "        print(f\"Evaluation report generated at {report_path}\")\n",
    "    else:\n",
    "        logger.warning(\"Failed to evaluate best model. Will try individual models.\")\n",
    "else:\n",
    "    logger.warning(\"Best model name not found. Will evaluate individual models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of the best model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "First, let's visualize the confusion matrix to understand the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix for test set\n",
    "y_true = raw_data['test']['y_true']\n",
    "y_pred = raw_data['test']['y_pred']\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Absolute counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted labels')\n",
    "axes[0].set_ylabel('True labels')\n",
    "axes[0].set_title('Confusion Matrix - Absolute Counts')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraudulent'])\n",
    "axes[0].set_yticklabels(['Legitimate', 'Fraudulent'])\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted labels')\n",
    "axes[1].set_ylabel('True labels')\n",
    "axes[1].set_title('Confusion Matrix - Normalized by True Labels')\n",
    "axes[1].set_xticklabels(['Legitimate', 'Fraudulent'])\n",
    "axes[1].set_yticklabels(['Legitimate', 'Fraudulent'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "Let's plot the ROC curve to evaluate the model's ability to distinguish between legitimate and fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for test set\n",
    "y_true = raw_data['test']['y_true']\n",
    "y_score = raw_data['test']['probabilities']\n",
    "\n",
    "if len(np.unique(y_true)) == 2:\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC curve is only applicable to binary classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n",
    "\n",
    "In fraud detection, precision and recall are often more important metrics because the classes are imbalanced. Let's plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve for test set\n",
    "y_true = raw_data['test']['y_true']\n",
    "y_score = raw_data['test']['probabilities']\n",
    "\n",
    "if len(np.unique(y_true)) == 2:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    average_precision = average_precision_score(y_true, y_score)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.step(recall, precision, color='darkorange', lw=2, where='post',\n",
    "             label=f'Precision-Recall curve (AP = {average_precision:.3f})')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='darkorange')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate class imbalance\n",
    "    class_counts = np.bincount(y_true)\n",
    "    fraud_ratio = class_counts[1] / len(y_true) if len(class_counts) > 1 else 0\n",
    "    print(f\"Class imbalance: {fraud_ratio:.2%} fraudulent transactions\")\n",
    "    print(f\"Random classifier baseline AP: {fraud_ratio:.4f}\")\n",
    "    print(f\"Model AP: {average_precision:.4f} (improvement: {average_precision/fraud_ratio:.2f}x)\")\n",
    "else:\n",
    "    print(\"Precision-Recall curve is only applicable to binary classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-specific Performance\n",
    "\n",
    "Let's analyze the model's performance for each class separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class-specific metrics\n",
    "test_metrics = metrics['test']\n",
    "\n",
    "print(f\"Classification Report for {best_model_name} on Test Set:\\n\")\n",
    "\n",
    "# Extract class-specific metrics\n",
    "classes = set()\n",
    "for k in test_metrics:\n",
    "    if '_' in k and k.split('_')[0].isdigit():\n",
    "        classes.add(int(k.split('_')[0]))\n",
    "\n",
    "if classes:\n",
    "    class_metrics = pd.DataFrame()\n",
    "    \n",
    "    for cls in sorted(classes):\n",
    "        cls_metrics = {}\n",
    "        cls_metrics['Class'] = 'Legitimate' if cls == 0 else 'Fraudulent'\n",
    "        cls_metrics['Precision'] = test_metrics.get(f\"{cls}_precision\", 'N/A')\n",
    "        cls_metrics['Recall'] = test_metrics.get(f\"{cls}_recall\", 'N/A')\n",
    "        cls_metrics['F1-Score'] = test_metrics.get(f\"{cls}_f1-score\", 'N/A')\n",
    "        cls_metrics['Support'] = test_metrics.get(f\"{cls}_support\", 'N/A')\n",
    "        \n",
    "        class_metrics = pd.concat([class_metrics, pd.DataFrame([cls_metrics])], ignore_index=True)\n",
    "    \n",
    "    print(class_metrics)\n",
    "    \n",
    "    # Add overall metrics\n",
    "    overall_metrics = pd.DataFrame([\n",
    "        {\n",
    "            'Class': 'Overall (accuracy)',\n",
    "            'Precision': 'N/A',\n",
    "            'Recall': 'N/A',\n",
    "            'F1-Score': test_metrics.get('accuracy', 'N/A'),\n",
    "            'Support': len(y_true)\n",
    "        },\n",
    "        {\n",
    "            'Class': 'Overall (weighted avg)',\n",
    "            'Precision': test_metrics.get('weighted avg_precision', 'N/A'),\n",
    "            'Recall': test_metrics.get('weighted avg_recall', 'N/A'),\n",
    "            'F1-Score': test_metrics.get('weighted avg_f1-score', 'N/A'),\n",
    "            'Support': len(y_true)\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nOverall metrics:\")\n",
    "    print(overall_metrics)\n",
    "    \n",
    "    # Print ROC AUC and PR AUC if available\n",
    "    print(\"\\nAdditional metrics:\")\n",
    "    if 'roc_auc' in test_metrics:\n",
    "        print(f\"ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "    if 'pr_auc' in test_metrics:\n",
    "        print(f\"PR AUC: {test_metrics['pr_auc']:.4f}\")\n",
    "else:\n",
    "    print(\"No class-specific metrics found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze the model's errors to understand where it's failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "y_true = raw_data['test']['y_true']\n",
    "y_pred = raw_data['test']['y_pred']\n",
    "probs = raw_data['test']['probabilities']\n",
    "\n",
    "# Create a DataFrame for error analysis\n",
    "error_df = pd.DataFrame({\n",
    "    'True_Label': y_true,\n",
    "    'Predicted_Label': y_pred,\n",
    "    'Probability': probs if len(np.unique(y_true)) == 2 else np.max(probs, axis=1),\n",
    "    'Is_Error': y_true != y_pred\n",
    "})\n",
    "\n",
    "# Calculate error rates\n",
    "overall_error_rate = error_df['Is_Error'].mean()\n",
    "\n",
    "# Error rates by class\n",
    "class_error_rates = {}\n",
    "for cls in np.unique(y_true):\n",
    "    class_mask = error_df['True_Label'] == cls\n",
    "    class_error_rate = error_df.loc[class_mask, 'Is_Error'].mean()\n",
    "    class_error_rates[cls] = class_error_rate\n",
    "\n",
    "print(f\"Overall error rate: {overall_error_rate:.4f} ({overall_error_rate*100:.2f}%)\")\n",
    "print(\"Error rates by class:\")\n",
    "for cls, rate in class_error_rates.items():\n",
    "    cls_name = 'Legitimate' if cls == 0 else 'Fraudulent'\n",
    "    print(f\"Class {cls} ({cls_name}): {rate:.4f} ({rate*100:.2f}%)\")\n",
    "\n",
    "# Analyze false positives and false negatives\n",
    "false_positives = error_df[(error_df['True_Label'] == 0) & (error_df['Predicted_Label'] == 1)]\n",
    "false_negatives = error_df[(error_df['True_Label'] == 1) & (error_df['Predicted_Label'] == 0)]\n",
    "\n",
    "print(f\"\\nFalse positives: {len(false_positives)} ({len(false_positives)/len(error_df):.4f})\")\n",
    "print(f\"False negatives: {len(false_negatives)} ({len(false_negatives)/len(error_df):.4f})\")\n",
    "\n",
    "# Distribution of probabilities for errors\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot histogram of probabilities for correct and incorrect predictions\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(error_df[~error_df['Is_Error']]['Probability'], bins=20, alpha=0.5, label='Correct', color='green')\n",
    "sns.histplot(error_df[error_df['Is_Error']]['Probability'], bins=20, alpha=0.5, label='Incorrect', color='red')\n",
    "plt.title('Distribution of Probabilities for Correct vs. Incorrect Predictions')\n",
    "plt.xlabel('Probability of Predicted Class')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Plot histogram of probabilities by true class\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(error_df[error_df['True_Label'] == 0]['Probability'], bins=20, alpha=0.5, label='True Legitimate', color='blue')\n",
    "sns.histplot(error_df[error_df['True_Label'] == 1]['Probability'], bins=20, alpha=0.5, label='True Fraudulent', color='red')\n",
    "plt.title('Distribution of Probabilities by True Class')\n",
    "plt.xlabel('Probability (of being Fraudulent)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal threshold for maximizing F1 score or minimizing the cost of misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold based on F1 score\n",
    "if len(np.unique(y_true)) == 2:\n",
    "    # Calculate precision, recall, and F1 score for different thresholds\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, probs)\n",
    "    \n",
    "    # Calculate F1 score for each threshold\n",
    "    f1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n",
    "    \n",
    "    # Find threshold with the highest F1 score\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "    \n",
    "    print(f\"Optimal threshold for F1 score: {best_threshold:.4f} (F1 = {best_f1:.4f})\")\n",
    "    \n",
    "    # Calculate metrics at the optimal threshold\n",
    "    y_pred_opt = (probs >= best_threshold).astype(int)\n",
    "    cm_opt = confusion_matrix(y_true, y_pred_opt)\n",
    "    tn, fp, fn, tp = cm_opt.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f\"Metrics at optimal threshold:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Plot precision-recall curve and optimal threshold\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(recalls, precisions, color='darkorange', lw=2)\n",
    "    plt.scatter(recalls[best_idx], precisions[best_idx], color='red', s=100, label=f'Optimal Threshold: {best_threshold:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve with Optimal Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot F1 score vs. threshold\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(thresholds, f1_scores, color='darkorange', lw=2)\n",
    "    plt.scatter(best_threshold, best_f1, color='red', s=100, label=f'Optimal Threshold: {best_threshold:.4f}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs. Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Threshold optimization is only applicable to binary classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the performance of all trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "model_names = ['gcn', 'sage', 'gat']\n",
    "\n",
    "# Store metrics for each model\n",
    "all_metrics = {}\n",
    "available_models = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name in model_names:\n",
    "    model_path = os.path.join('models', f'{model_name}_best.pt')\n",
    "    if os.path.exists(model_path):\n",
    "        logger.info(f\"Evaluating {model_name} model\")\n",
    "        \n",
    "        try:\n",
    "            # Load and evaluate model\n",
    "            metrics, raw_data = load_and_evaluate_model(model_name, data, split_idx, device)\n",
    "            \n",
    "            if metrics is not None:\n",
    "                # Store metrics\n",
    "                all_metrics[model_name] = metrics\n",
    "                available_models.append(model_name)\n",
    "                \n",
    "                # Generate evaluation report\n",
    "                generate_evaluation_report(metrics, raw_data, model_name)\n",
    "            else:\n",
    "                logger.warning(f\"Failed to evaluate {model_name} model\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating {model_name} model: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"{model_name} model weights not found at {model_path}\")\n",
    "\n",
    "# Compare models if multiple models are available\n",
    "if len(available_models) > 1:\n",
    "    logger.info(\"Comparing model performance\")\n",
    "    \n",
    "    # Create a DataFrame for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in available_models:\n",
    "        model_metrics = all_metrics[model_name]['test']\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': model_metrics.get('accuracy', float('nan')),\n",
    "            'Precision': model_metrics.get('weighted avg_precision', float('nan')),\n",
    "            'Recall': model_metrics.get('weighted avg_recall', float('nan')),\n",
    "            'F1-Score': model_metrics.get('weighted avg_f1-score', float('nan')),\n",
    "            'ROC AUC': model_metrics.get('roc_auc', float('nan')),\n",
    "            'PR AUC': model_metrics.get('pr_auc', float('nan')),\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"Model comparison (test set performance):\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    if not comparison_df['ROC AUC'].isna().all():\n",
    "        metrics_to_plot.extend(['ROC AUC', 'PR AUC'])\n",
    "    \n",
    "    # Reshape for plotting\n",
    "    comparison_melted = pd.melt(comparison_df, id_vars=['Model'], value_vars=metrics_to_plot, \n",
    "                               var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(1, len(metrics_to_plot), figsize=(20, 6))\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        metric_data = comparison_melted[comparison_melted['Metric'] == metric]\n",
    "        sns.barplot(x='Model', y='Value', data=metric_data, ax=axs[i], palette='viridis')\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_ylim([0, 1])\n",
    "        axs[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate comparison report\n",
    "    comparison_report_dir = os.path.join('reports', 'comparison')\n",
    "    os.makedirs(comparison_report_dir, exist_ok=True)\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    comparison_df.to_csv(os.path.join(comparison_report_dir, 'model_comparison.csv'), index=False)\n",
    "    \n",
    "    # Create markdown report\n",
    "    report_md = \"# Model Comparison Report\\n\\n\"\n",
    "    \n",
    "    # Add test performance comparison\n",
    "    report_md += \"## Test Set Performance\\n\\n\"\n",
    "    \n",
    "    # Create table header\n",
    "    report_md += \"| Model | \" + \" | \".join(metrics_to_plot) + \" |\\n\"\n",
    "    report_md += \"|-------|\" + \"|\".join([\"---\" for _ in metrics_to_plot]) + \"|\\n\"\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        # Add model name\n",
    "        table_row = f\"| {row['Model']} |\"\n",
    "        \n",
    "        # Add metrics\n",
    "        for metric in metrics_to_plot:\n",
    "            value = row[metric]\n",
    "            if isinstance(value, float) and not np.isnan(value):\n",
    "                value = f\"{value:.4f}\"\n",
    "            else:\n",
    "                value = 'N/A'\n",
    "            table_row += f\" {value} |\"\n",
    "        \n",
    "        report_md += table_row + \"\\n\"\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(comparison_report_dir, 'model_comparison.md')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_md)\n",
    "    \n",
    "    print(f\"\\nModel comparison report saved to {report_path}\")\n",
    "else:\n",
    "    print(\"Not enough models available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Evaluation Report\n",
    "\n",
    "We've already generated comprehensive evaluation reports for each model. Let's summarize the key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key findings\n",
    "if best_model_name is not None and best_model_name in all_metrics:\n",
    "    test_metrics = all_metrics[best_model_name]['test']\n",
    "    \n",
    "    print(f\"Summary of {best_model_name.upper()} model performance on test set:\")\n",
    "    print(f\"Accuracy: {test_metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"Weighted Precision: {test_metrics.get('weighted avg_precision', 'N/A'):.4f}\")\n",
    "    print(f\"Weighted Recall: {test_metrics.get('weighted avg_recall', 'N/A'):.4f}\")\n",
    "    print(f\"Weighted F1-Score: {test_metrics.get('weighted avg_f1-score', 'N/A'):.4f}\")\n",
    "    \n",
    "    if 'roc_auc' in test_metrics:\n",
    "        print(f\"ROC AUC: {test_metrics.get('roc_auc', 'N/A'):.4f}\")\n",
    "    if 'pr_auc' in test_metrics:\n",
    "        print(f\"PR AUC: {test_metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "        \n",
    "    # Class-specific metrics\n",
    "    print(\"\\nClass-specific metrics:\")\n",
    "    for cls in sorted(classes):\n",
    "        cls_name = 'Legitimate' if cls == 0 else 'Fraudulent'\n",
    "        print(f\"Class {cls} ({cls_name}):\")\n",
    "        print(f\"  Precision: {test_metrics.get(f'{cls}_precision', 'N/A'):.4f}\")\n",
    "        print(f\"  Recall: {test_metrics.get(f'{cls}_recall', 'N/A'):.4f}\")\n",
    "        print(f\"  F1-Score: {test_metrics.get(f'{cls}_f1-score', 'N/A'):.4f}\")\n",
    "        print(f\"  Support: {test_metrics.get(f'{cls}_support', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No best model metrics available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
