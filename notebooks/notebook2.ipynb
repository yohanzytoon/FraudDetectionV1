{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain Fraud Detection: Feature Engineering\n",
    "\n",
    "This notebook focuses on creating new features to enhance the fraud detection model. We'll develop features from two main sources:\n",
    "\n",
    "1. **Transaction Features**: From the raw features in the nodes dataset\n",
    "2. **Graph Features**: Based on the network structure from the edges dataset\n",
    "\n",
    "Our goal is to combine these feature sets to create a comprehensive representation for the Graph Neural Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directory for processed data\n",
    "os.makedirs('../data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "nodes_path = '../data/raw/nodes.csv'\n",
    "edges_path = '../data/raw/edges.csv'\n",
    "\n",
    "df_nodes = pd.read_csv(nodes_path)\n",
    "df_edges = pd.read_csv(edges_path)\n",
    "\n",
    "print(f\"Node data shape: {df_nodes.shape}\")\n",
    "print(f\"Edge data shape: {df_edges.shape}\")\n",
    "\n",
    "# Create a mapping of node IDs to indices\n",
    "id2idx = {df_nodes.iloc[i, 0]: i for i in range(len(df_nodes))}\n",
    "print(f\"Created mapping from transaction IDs to indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for low-variance features\n",
    "feature_cols = df_nodes.columns[2:]  # Exclude ID and class\n",
    "feature_variance = df_nodes[feature_cols].var()\n",
    "\n",
    "low_var_threshold = 0.01\n",
    "low_var_features = feature_variance[feature_variance < low_var_threshold].index.tolist()\n",
    "\n",
    "print(f\"Number of low-variance features (variance < {low_var_threshold}): {len(low_var_features)}\")\n",
    "print(f\"Removing these features from the dataset\")\n",
    "\n",
    "# Remove low-variance features\n",
    "df_nodes_clean = df_nodes.drop(columns=low_var_features)\n",
    "print(f\"Original feature count: {len(feature_cols)}\")\n",
    "print(f\"Cleaned feature count: {len(df_nodes_clean.columns) - 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Transaction-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transaction features from the raw data\n",
    "transaction_features = df_nodes_clean.iloc[:, 2:].values\n",
    "print(f\"Transaction features shape: {transaction_features.shape}\")\n",
    "\n",
    "# Save original feature names for interpretation\n",
    "transaction_feature_names = df_nodes_clean.columns[2:].tolist()\n",
    "\n",
    "# Review sample features\n",
    "print(\"Sample transaction features (first 5 rows, first 5 features):\")\n",
    "print(transaction_features[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance with mutual information\n",
    "labels = df_nodes_clean['class'].values\n",
    "selector = SelectKBest(mutual_info_classif, k='all')\n",
    "selector.fit(transaction_features, labels)\n",
    "\n",
    "# Create DataFrame of feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': transaction_feature_names,\n",
    "    'Mutual Information': selector.scores_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Mutual Information', ascending=False)\n",
    "\n",
    "# Display top 20 important features\n",
    "print(\"Top 20 features by mutual information with the target:\")\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Mutual Information', y='Feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Features by Mutual Information with Target', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Engineer Graph-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graph from edges\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all edges to the graph\n",
    "edge_count = 0\n",
    "for _, row in df_edges.iterrows():\n",
    "    source_id, target_id = row[0], row[1]\n",
    "    # Only add edges between nodes present in df_nodes\n",
    "    if source_id in id2idx and target_id in id2idx:\n",
    "        G.add_edge(source_id, target_id)\n",
    "        edge_count += 1\n",
    "\n",
    "print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"Added {edge_count} out of {len(df_edges)} edges from the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic node centrality metrics\n",
    "print(\"Calculating node centrality metrics...\")\n",
    "\n",
    "# In-degree and out-degree centrality\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "\n",
    "# PageRank (with modifications for large graphs)\n",
    "try:\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    print(\"PageRank failed to converge, using a simplified calculation\")\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=50, tol=1e-3)\n",
    "\n",
    "# HITS (hub and authority scores)\n",
    "try:\n",
    "    hubs, authorities = nx.hits(G, max_iter=100)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    print(\"HITS algorithm failed to converge, using a simplified calculation\")\n",
    "    hubs, authorities = nx.hits(G, max_iter=50, tol=1e-3)\n",
    "\n",
    "print(\"Centrality metrics calculation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate local graph metrics (more computationally efficient)\n",
    "print(\"Calculating local graph metrics...\")\n",
    "\n",
    "# Raw in-degree and out-degree\n",
    "in_degrees = dict(G.in_degree())\n",
    "out_degrees = dict(G.out_degree())\n",
    "\n",
    "# Successors and predecessors count\n",
    "successors_count = {node: len(list(G.successors(node))) for node in G.nodes()}\n",
    "predecessors_count = {node: len(list(G.predecessors(node))) for node in G.nodes()}\n",
    "\n",
    "# Calculate clustering coefficient for a sample of nodes (for undirected version)\n",
    "G_undirected = G.to_undirected()\n",
    "clustering = nx.clustering(G_undirected)\n",
    "\n",
    "print(\"Local graph metrics calculation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble graph features into a DataFrame\n",
    "node_ids = df_nodes_clean['txId'].values\n",
    "graph_features = np.zeros((len(node_ids), 8))\n",
    "\n",
    "for i, node_id in enumerate(node_ids):\n",
    "    if node_id in G.nodes():\n",
    "        # Centrality measures\n",
    "        graph_features[i, 0] = in_degree_centrality.get(node_id, 0)\n",
    "        graph_features[i, 1] = out_degree_centrality.get(node_id, 0)\n",
    "        graph_features[i, 2] = pagerank.get(node_id, 0)\n",
    "        graph_features[i, 3] = hubs.get(node_id, 0)\n",
    "        graph_features[i, 4] = authorities.get(node_id, 0)\n",
    "        \n",
    "        # Local metrics\n",
    "        graph_features[i, 5] = in_degrees.get(node_id, 0)\n",
    "        graph_features[i, 6] = out_degrees.get(node_id, 0)\n",
    "        graph_features[i, 7] = clustering.get(node_id, 0)\n",
    "\n",
    "print(f\"Created graph features matrix with shape {graph_features.shape}\")\n",
    "\n",
    "# Define feature names for interpretation\n",
    "graph_feature_names = [\n",
    "    'in_degree_centrality',\n",
    "    'out_degree_centrality',\n",
    "    'pagerank',\n",
    "    'hub_score',\n",
    "    'authority_score',\n",
    "    'in_degree',\n",
    "    'out_degree',\n",
    "    'clustering_coefficient'\n",
    "]\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_graph_features = pd.DataFrame(graph_features, columns=graph_feature_names)\n",
    "df_graph_features['class'] = df_nodes_clean['class'].values\n",
    "\n",
    "print(\"Basic statistics of graph features:\")\n",
    "df_graph_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph features by class\n",
    "graph_features_by_class = df_graph_features.groupby('class')[graph_feature_names].mean()\n",
    "\n",
    "print(\"Average graph features by class:\")\n",
    "graph_features_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of graph features by class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(graph_feature_names):\n",
    "    sns.boxplot(x='class', y=feature, data=df_graph_features, palette=['#4285F4', '#EA4335'], ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Class', fontsize=12)\n",
    "    axes[i].set_xlabel('Class (0=Legitimate, 1=Fraudulent)', fontsize=10)\n",
    "    axes[i].set_ylabel(feature, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance for graph features\n",
    "selector_graph = SelectKBest(mutual_info_classif, k='all')\n",
    "selector_graph.fit(graph_features, labels)\n",
    "\n",
    "# Create DataFrame of feature importances\n",
    "graph_importance = pd.DataFrame({\n",
    "    'Feature': graph_feature_names,\n",
    "    'Mutual Information': selector_graph.scores_\n",
    "})\n",
    "graph_importance = graph_importance.sort_values('Mutual Information', ascending=False)\n",
    "\n",
    "print(\"Graph features ranked by mutual information with the target:\")\n",
    "graph_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Engineer Additional Features (Optional)\n",
    "\n",
    "Let's add some additional features that might be useful for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fraud proximity features\n",
    "# These measure how closely a node is connected to fraudulent nodes\n",
    "\n",
    "print(\"Calculating fraud proximity features...\")\n",
    "\n",
    "# Get fraud nodes\n",
    "fraud_nodes = set(df_nodes_clean[df_nodes_clean['class'] == 1]['txId'].values)\n",
    "print(f\"Number of fraud nodes: {len(fraud_nodes)}\")\n",
    "\n",
    "# Initialize features\n",
    "fraud_proximity_features = np.zeros((len(node_ids), 2))\n",
    "\n",
    "for i, node_id in enumerate(node_ids):\n",
    "    if node_id in G.nodes():\n",
    "        # Count fraud nodes in predecessors (incoming connections)\n",
    "        predecessors = set(G.predecessors(node_id))\n",
    "        fraud_predecessors = predecessors.intersection(fraud_nodes)\n",
    "        if predecessors:\n",
    "            fraud_proximity_features[i, 0] = len(fraud_predecessors) / len(predecessors)\n",
    "        \n",
    "        # Count fraud nodes in successors (outgoing connections)\n",
    "        successors = set(G.successors(node_id))\n",
    "        fraud_successors = successors.intersection(fraud_nodes)\n",
    "        if successors:\n",
    "            fraud_proximity_features[i, 1] = len(fraud_successors) / len(successors)\n",
    "\n",
    "print(f\"Created fraud proximity features with shape {fraud_proximity_features.shape}\")\n",
    "\n",
    "# Define feature names\n",
    "fraud_proximity_names = [\n",
    "    'fraud_predecessor_ratio',\n",
    "    'fraud_successor_ratio'\n",
    "]\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df_fraud_proximity = pd.DataFrame(fraud_proximity_features, columns=fraud_proximity_names)\n",
    "df_fraud_proximity['class'] = df_nodes_clean['class'].values\n",
    "\n",
    "print(\"Basic statistics of fraud proximity features:\")\n",
    "df_fraud_proximity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze fraud proximity features by class\n",
    "fraud_proximity_by_class = df_fraud_proximity.groupby('class')[fraud_proximity_names].mean()\n",
    "\n",
    "print(\"Average fraud proximity features by class:\")\n",
    "fraud_proximity_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fraud proximity features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for i, feature in enumerate(fraud_proximity_names):\n",
    "    sns.boxplot(x='class', y=feature, data=df_fraud_proximity, palette=['#4285F4', '#EA4335'], ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Class', fontsize=14)\n",
    "    axes[i].set_xlabel('Class (0=Legitimate, 1=Fraudulent)', fontsize=12)\n",
    "    axes[i].set_ylabel(feature, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all feature sets\n",
    "all_features = np.hstack((transaction_features, graph_features, fraud_proximity_features))\n",
    "all_feature_names = transaction_feature_names + graph_feature_names + fraud_proximity_names\n",
    "\n",
    "print(f\"Combined features shape: {all_features.shape}\")\n",
    "print(f\"Number of features: {len(all_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(all_features)\n",
    "\n",
    "print(\"Features normalized to zero mean and unit variance\")\n",
    "print(f\"Scaled features shape: {scaled_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances for combined features\n",
    "selector_all = SelectKBest(mutual_info_classif, k='all')\n",
    "selector_all.fit(scaled_features, labels)\n",
    "\n",
    "# Create DataFrame of feature importances\n",
    "all_importance = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Mutual Information': selector_all.scores_\n",
    "})\n",
    "all_importance = all_importance.sort_values('Mutual Information', ascending=False)\n",
    "\n",
    "print(\"Top 20 features by mutual information with the target:\")\n",
    "all_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top combined features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Mutual Information', y='Feature', data=all_importance.head(20))\n",
    "plt.title('Top 20 Combined Features by Mutual Information with Target', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Edge Index for PyTorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build edge index for PyTorch Geometric\n",
    "print(\"Building edge index for PyTorch Geometric...\")\n",
    "\n",
    "edges = []\n",
    "skipped = 0\n",
    "\n",
    "for _, row in df_edges.iterrows():\n",
    "    source_id, target_id = row[0], row[1]\n",
    "    \n",
    "    # Only include edges where both nodes are in the node dataset\n",
    "    if source_id in id2idx and target_id in id2idx:\n",
    "        source_idx = id2idx[source_id]\n",
    "        target_idx = id2idx[target_id]\n",
    "        edges.append([source_idx, target_idx])\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "print(f\"Created edge index with shape {edge_index.shape}\")\n",
    "print(f\"Skipped {skipped} edges due to missing nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed features\n",
    "np.save('../data/processed/features.npy', scaled_features)\n",
    "with open('../data/processed/feature_names.txt', 'w') as f:\n",
    "    for name in all_feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Save labels\n",
    "np.save('../data/processed/labels.npy', labels)\n",
    "\n",
    "# Save edge index\n",
    "torch.save(edge_index, '../data/processed/edge_index.pt')\n",
    "\n",
    "# Save feature importances\n",
    "all_importance.to_csv('../data/processed/feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Saved processed data to ../data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Geometric Data object\n",
    "features_tensor = torch.FloatTensor(scaled_features)\n",
    "labels_tensor = torch.LongTensor(labels)\n",
    "\n",
    "data = Data(x=features_tensor, edge_index=edge_index, y=labels_tensor)\n",
    "print(f\"Created PyTorch Geometric Data object: {data}\")\n",
    "\n",
    "# Save the complete Data object\n",
    "torch.save(data, '../data/processed/data.pt')\n",
    "print(\"Saved PyTorch Geometric Data object to ../data/processed/data.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Feature Engineering\n",
    "\n",
    "In this notebook, we've created a comprehensive feature set for blockchain fraud detection by combining:\n",
    "\n",
    "1. **Transaction Features**:\n",
    "   - Cleaned and normalized the original features\n",
    "   - Removed low-variance features\n",
    "   - Identified the most informative features\n",
    "\n",
    "2. **Graph Features**:\n",
    "   - Centrality metrics (degree, PageRank, HITS)\n",
    "   - Local graph structure (clustering coefficient)\n",
    "   - Raw topological features (in/out degrees)\n",
    "\n",
    "3. **Fraud Proximity Features**:\n",
    "   - Ratio of fraudulent predecessors\n",
    "   - Ratio of fraudulent successors\n",
    "\n",
    "The combined feature set has high discriminative power for fraud detection, as seen in our feature importance analysis. We've prepared the data in the format required for PyTorch Geometric, which will be used to build our GNN models in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
