{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain Fraud Detection: Fraud Case Study\n",
    "\n",
    "This notebook focuses on analyzing specific fraudulent transactions and understanding the patterns that our GNN model has learned. We'll investigate the characteristics of different types of fraud and visualize their network structures to gain deeper insights into blockchain fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import warnings\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directory for case study results\n",
    "os.makedirs('../reports/case_study', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "nodes_path = '../data/raw/nodes.csv'\n",
    "edges_path = '../data/raw/edges.csv'\n",
    "\n",
    "try:\n",
    "    df_nodes = pd.read_csv(nodes_path)\n",
    "    df_edges = pd.read_csv(edges_path)\n",
    "    print(f\"Loaded raw data from CSV files\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Raw data files not found. Please make sure {nodes_path} and {edges_path} exist.\")\n",
    "\n",
    "print(f\"Nodes shape: {df_nodes.shape}\")\n",
    "print(f\"Edges shape: {df_edges.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "try:\n",
    "    # Try loading the complete Data object\n",
    "    data = torch.load('../data/processed/data.pt')\n",
    "    print(f\"Loaded PyTorch Geometric Data object: {data}\")\n",
    "except FileNotFoundError:\n",
    "    # If not found, load individual components\n",
    "    features = np.load('../data/processed/features.npy')\n",
    "    labels = np.load('../data/processed/labels.npy')\n",
    "    edge_index = torch.load('../data/processed/edge_index.pt')\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.FloatTensor(features)\n",
    "    y = torch.LongTensor(labels)\n",
    "    \n",
    "    # Create Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    print(f\"Created PyTorch Geometric Data object from components\")\n",
    "\n",
    "# Load feature names if available\n",
    "try:\n",
    "    with open('../data/processed/feature_names.txt', 'r') as f:\n",
    "        feature_names = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Loaded {len(feature_names)} feature names\")\n",
    "except FileNotFoundError:\n",
    "    feature_names = [f'Feature_{i}' for i in range(data.num_features)]\n",
    "    print(f\"Created generic feature names\")\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCN model\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                 dropout=0.5, batch_norm=True, residual=True):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = torch.nn.ModuleList([torch.nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev\n",
    "            \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def get_embeddings(self, x, edge_index, layer=-2):\n",
    "        \"\"\"Extract embeddings from a specific layer\"\"\"\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            if i == self.num_layers + layer:  # If this is the requested layer\n",
    "                return h\n",
    "                \n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev\n",
    "            \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Return embeddings from before the final layer by default\n",
    "        if layer == -2:\n",
    "            return h\n",
    "        \n",
    "        # Or return final layer output if requested\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "try:\n",
    "    with open('../models/best_model_name.txt', 'r') as f:\n",
    "        best_model_name = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    # Default to GCN if model name is not found\n",
    "    best_model_name = 'GCN'\n",
    "\n",
    "# Initialize model with appropriate architecture\n",
    "input_dim = data.num_features\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "\n",
    "# For simplicity, we'll use GCN model for this case study\n",
    "model = GCNModel(input_dim, hidden_dim, output_dim, num_layers=3).to(device)\n",
    "model_path = '../models/best_model.pt'\n",
    "\n",
    "# Try to load the model parameters\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    # Try model-specific path\n",
    "    model_specific_path = f\"../models/{best_model_name.lower()}_best.pt\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_specific_path, map_location=device))\n",
    "        print(f\"Loaded model from {model_specific_path}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not find model file at {model_path} or {model_specific_path}\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Predictions and Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all nodes\n",
    "with torch.no_grad():\n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Get probabilities and predictions\n",
    "    probs = torch.exp(out)\n",
    "    preds = out.argmax(dim=1)\n",
    "    \n",
    "    # Get embeddings from the second-to-last layer\n",
    "    embeddings = model.get_embeddings(data.x, data.edge_index, layer=-2)\n",
    "\n",
    "# Move to CPU for processing\n",
    "true_labels = data.y.cpu().numpy()\n",
    "predicted_labels = preds.cpu().numpy()\n",
    "fraud_probs = probs[:, 1].cpu().numpy()\n",
    "node_embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "print(f\"Generated predictions and embeddings for {len(predicted_labels)} nodes\")\n",
    "print(f\"Node embeddings shape: {node_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from node indices to original transaction IDs\n",
    "node_indices = np.arange(len(df_nodes))\n",
    "node_ids = df_nodes.iloc[:, 0].values  # First column contains the transaction IDs\n",
    "idx_to_id = dict(zip(node_indices, node_ids))\n",
    "\n",
    "# Create a results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'node_idx': node_indices,\n",
    "    'tx_id': node_ids,\n",
    "    'true_label': true_labels,\n",
    "    'predicted_label': predicted_labels,\n",
    "    'fraud_probability': fraud_probs,\n",
    "    'correct_prediction': true_labels == predicted_labels\n",
    "})\n",
    "\n",
    "# Add time step information if available\n",
    "if 'time_step' in df_nodes.columns:\n",
    "    results_df['time_step'] = df_nodes['time_step'].values\n",
    "\n",
    "# Print basic statistics\n",
    "print(\"Prediction Results Summary:\")\n",
    "print(f\"Total nodes: {len(results_df)}\")\n",
    "print(f\"Actual fraudulent nodes: {results_df['true_label'].sum()}\")\n",
    "print(f\"Predicted fraudulent nodes: {results_df['predicted_label'].sum()}\")\n",
    "print(f\"Correctly predicted nodes: {results_df['correct_prediction'].sum()}\")\n",
    "print(f\"Accuracy: {results_df['correct_prediction'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Different Types of Fraud\n",
    "\n",
    "Let's try to identify different patterns of fraudulent behavior by clustering the embeddings of fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings of actual fraudulent transactions\n",
    "fraud_idx = np.where(true_labels == 1)[0]\n",
    "fraud_embeddings = node_embeddings[fraud_idx]\n",
    "fraud_results = results_df[results_df['true_label'] == 1].copy()\n",
    "\n",
    "print(f\"Analyzing {len(fraud_idx)} fraudulent transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to visualize fraud embeddings in 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(fraud_embeddings) - 1), n_iter=1000)\n",
    "fraud_embeddings_2d = tsne.fit_transform(fraud_embeddings)\n",
    "\n",
    "# Create a plotting DataFrame\n",
    "fraud_plot_df = pd.DataFrame({\n",
    "    'x': fraud_embeddings_2d[:, 0],\n",
    "    'y': fraud_embeddings_2d[:, 1],\n",
    "    'tx_id': fraud_results['tx_id'].values,\n",
    "    'correct_prediction': fraud_results['correct_prediction'].values,\n",
    "    'fraud_probability': fraud_results['fraud_probability'].values\n",
    "})\n",
    "\n",
    "# Visualize the fraud embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(fraud_plot_df['x'], fraud_plot_df['y'], \n",
    "                       c=fraud_plot_df['fraud_probability'], cmap='Reds', \n",
    "                       alpha=0.7, s=50, edgecolors='w')\n",
    "plt.colorbar(scatter, label='Fraud Probability')\n",
    "plt.title('t-SNE Visualization of Fraudulent Transaction Embeddings', fontsize=15)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/case_study/fraud_embeddings.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use clustering to identify different fraud patterns\n",
    "# Let's try DBSCAN for density-based clustering\n",
    "dbscan = DBSCAN(eps=3.0, min_samples=5)  # Adjust parameters based on your data\n",
    "fraud_clusters = dbscan.fit_predict(fraud_embeddings_2d)\n",
    "\n",
    "# Add cluster information to the DataFrame\n",
    "fraud_plot_df['cluster'] = fraud_clusters\n",
    "\n",
    "# Count points per cluster\n",
    "n_clusters = len(set(fraud_clusters)) - (1 if -1 in fraud_clusters else 0)\n",
    "n_noise = list(fraud_clusters).count(-1)\n",
    "print(f\"DBSCAN identified {n_clusters} clusters, with {n_noise} noise points\")\n",
    "\n",
    "# If DBSCAN doesn't work well, try KMeans as an alternative\n",
    "if n_clusters <= 1:\n",
    "    print(\"DBSCAN clustering not effective, trying KMeans instead\")\n",
    "    n_clusters = min(5, len(fraud_embeddings))  # Limit to 5 clusters or fewer\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    fraud_clusters = kmeans.fit_predict(fraud_embeddings_2d)\n",
    "    fraud_plot_df['cluster'] = fraud_clusters\n",
    "    print(f\"KMeans identified {n_clusters} clusters\")\n",
    "\n",
    "# Create a colormap for clusters\n",
    "cmap = plt.cm.get_cmap('tab10', n_clusters)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot each cluster with a different color\n",
    "for cluster_id in range(-1, n_clusters):  # -1 is for noise points in DBSCAN\n",
    "    cluster_points = fraud_plot_df[fraud_plot_df['cluster'] == cluster_id]\n",
    "    \n",
    "    if cluster_id == -1:\n",
    "        # Noise points in black\n",
    "        plt.scatter(cluster_points['x'], cluster_points['y'], color='black', \n",
    "                    alpha=0.5, s=30, label=f'Noise ({len(cluster_points)} points)')\n",
    "    else:\n",
    "        # Cluster points in color\n",
    "        plt.scatter(cluster_points['x'], cluster_points['y'], color=cmap(cluster_id), \n",
    "                    alpha=0.7, s=50, label=f'Cluster {cluster_id} ({len(cluster_points)} points)')\n",
    "\n",
    "plt.title('Fraud Patterns: Clustered Embeddings', fontsize=15)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/case_study/fraud_clusters.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the relationship between clusters and correct predictions\n",
    "cluster_stats = fraud_plot_df.groupby('cluster').agg({\n",
    "    'tx_id': 'count',\n",
    "    'correct_prediction': 'mean',\n",
    "    'fraud_probability': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "cluster_stats.columns = ['Cluster', 'Count', 'Detection_Rate', 'Avg_Probability']\n",
    "cluster_stats = cluster_stats.sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"Fraud Cluster Statistics:\")\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cluster statistics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot detection rate by cluster\n",
    "ax1.bar(cluster_stats['Cluster'].astype(str), cluster_stats['Detection_Rate'], color='teal')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('Cluster', fontsize=12)\n",
    "ax1.set_ylabel('Detection Rate', fontsize=12)\n",
    "ax1.set_title('Fraud Detection Rate by Cluster', fontsize=14)\n",
    "for i, v in enumerate(cluster_stats['Detection_Rate']):\n",
    "    ax1.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "\n",
    "# Plot average probability by cluster\n",
    "ax2.bar(cluster_stats['Cluster'].astype(str), cluster_stats['Avg_Probability'], color='purple')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_xlabel('Cluster', fontsize=12)\n",
    "ax2.set_ylabel('Average Fraud Probability', fontsize=12)\n",
    "ax2.set_title('Average Fraud Probability by Cluster', fontsize=14)\n",
    "for i, v in enumerate(cluster_stats['Avg_Probability']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/case_study/cluster_statistics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Network Analysis of Fraud Patterns\n",
    "\n",
    "Let's examine the network structure of each fraud cluster to understand different fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NetworkX graph from the edges\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add all edges\n",
    "for _, row in df_edges.iterrows():\n",
    "    G.add_edge(row[0], row[1])\n",
    "\n",
    "print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from node/transaction ID to cluster\n",
    "tx_id_to_cluster = dict(zip(fraud_plot_df['tx_id'], fraud_plot_df['cluster']))\n",
    "\n",
    "# Function to extract a subgraph around a transaction ID\n",
    "def extract_transaction_neighborhood(G, tx_id, depth=1):\n",
    "    \"\"\"Extract a subgraph of nodes within 'depth' hops from tx_id\"\"\"\n",
    "    # Start with the transaction node\n",
    "    nodes = {tx_id}\n",
    "    frontier = {tx_id}\n",
    "    \n",
    "    # Add predecessors and successors up to depth\n",
    "    for _ in range(depth):\n",
    "        new_frontier = set()\n",
    "        for node in frontier:\n",
    "            # Add predecessors (incoming edges)\n",
    "            if node in G:\n",
    "                predecessors = set(G.predecessors(node))\n",
    "                new_frontier.update(predecessors)\n",
    "                \n",
    "                # Add successors (outgoing edges)\n",
    "                successors = set(G.successors(node))\n",
    "                new_frontier.update(successors)\n",
    "        \n",
    "        # Update nodes and frontier\n",
    "        nodes.update(new_frontier)\n",
    "        frontier = new_frontier\n",
    "    \n",
    "    # Extract the subgraph\n",
    "    return G.subgraph(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize a transaction network with node colors\n",
    "def visualize_transaction_network(subgraph, central_node=None, title=\"Transaction Network\", \n",
    "                                  output_path=None, figsize=(12, 10)):\n",
    "    \"\"\"Visualize a transaction subgraph with coloring based on fraud labels\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create a position layout\n",
    "    pos = nx.spring_layout(subgraph, seed=42)\n",
    "    \n",
    "    # Prepare node colors and sizes\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    \n",
    "    for node in subgraph.nodes():\n",
    "        # Highlight the central node\n",
    "        if node == central_node:\n",
    "            node_colors.append('red')\n",
    "            node_sizes.append(500)\n",
    "        # Color fraudulent nodes based on their cluster if known\n",
    "        elif node in tx_id_to_cluster:\n",
    "            cluster = tx_id_to_cluster[node]\n",
    "            if cluster == -1:  # Noise points\n",
    "                node_colors.append('gray')\n",
    "            else:\n",
    "                node_colors.append(plt.cm.tab10(cluster))\n",
    "            node_sizes.append(300)\n",
    "        # Color known legitimate nodes\n",
    "        elif node in idx_to_id.values() and node in df_nodes.iloc[:, 0].values:\n",
    "            # Get the row index for this node ID\n",
    "            node_idx = df_nodes.index[df_nodes.iloc[:, 0] == node][0]\n",
    "            if df_nodes.iloc[node_idx, 1] == 0:  # Check if it's legitimate\n",
    "                node_colors.append('lightblue')\n",
    "                node_sizes.append(200)\n",
    "            else:  # It's fraudulent but not clustered\n",
    "                node_colors.append('orange')\n",
    "                node_sizes.append(300)\n",
    "        # Unknown or uncategorized nodes\n",
    "        else:\n",
    "            node_colors.append('lightgray')\n",
    "            node_sizes.append(150)\n",
    "    \n",
    "    # Draw the network\n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                          alpha=0.8, edgecolors='black', linewidths=0.5)\n",
    "    nx.draw_networkx_edges(subgraph, pos, alpha=0.4, arrows=True)\n",
    "    \n",
    "    # Add minimal labels to avoid clutter\n",
    "    if len(subgraph) < 50:  # Only add labels for small graphs\n",
    "        labels = {}\n",
    "        for node in subgraph.nodes():\n",
    "            if node == central_node:\n",
    "                labels[node] = f\"ID: {node}\"\n",
    "            elif node in tx_id_to_cluster:\n",
    "                labels[node] = f\"F{tx_id_to_cluster[node]}\"\n",
    "            elif len(str(node)) > 10:\n",
    "                labels[node] = str(node)[:3] + '...' + str(node)[-3:]\n",
    "            else:\n",
    "                labels[node] = str(node)\n",
    "        nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)\n",
    "    \n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Central Transaction'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', markersize=10, label='Legitimate'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Fraudulent (Unclustered)'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', markersize=10, label='Noise Points')\n",
    "    ]\n",
    "    \n",
    "    # Add cluster colors to legend\n",
    "    for cluster in range(n_clusters):\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab10(cluster), \n",
    "                   markersize=10, label=f'Fraud Cluster {cluster}')\n",
    "        )\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path provided\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze representative transactions from each cluster\n",
    "for cluster_id in sorted(cluster_stats['Cluster'].unique()):\n",
    "    if cluster_id == -1 and n_noise < 5:  # Skip noise if there are few noise points\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nAnalyzing Cluster {cluster_id}:\")\n",
    "    \n",
    "    # Get transactions in this cluster\n",
    "    cluster_txs = fraud_plot_df[fraud_plot_df['cluster'] == cluster_id]['tx_id'].values\n",
    "    \n",
    "    if len(cluster_txs) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Select a representative transaction\n",
    "    if len(cluster_txs) <= 5:\n",
    "        # For small clusters, just pick the first one\n",
    "        tx_id = cluster_txs[0]\n",
    "    else:\n",
    "        # For larger clusters, pick one with a high fraud probability\n",
    "        probs = fraud_plot_df[fraud_plot_df['cluster'] == cluster_id]['fraud_probability'].values\n",
    "        idx = np.argsort(probs)[-3]  # Get the third highest probability\n",
    "        tx_id = cluster_txs[idx]\n",
    "    \n",
    "    print(f\"Representative transaction ID: {tx_id}\")\n",
    "    \n",
    "    # Extract and visualize the transaction's neighborhood\n",
    "    try:\n",
    "        subgraph = extract_transaction_neighborhood(G, tx_id, depth=1)\n",
    "        print(f\"Extracted subgraph with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
    "        \n",
    "        # Limit to smaller graphs for visualization\n",
    "        if subgraph.number_of_nodes() > 50:\n",
    "            print(f\"Subgraph is too large. Sampling a smaller neighborhood.\")\n",
    "            subgraph = extract_transaction_neighborhood(G, tx_id, depth=0)\n",
    "            if subgraph.number_of_nodes() > 50:\n",
    "                # If still too large, just show immediate neighbors\n",
    "                neighbors = list(G.predecessors(tx_id)) + list(G.successors(tx_id))\n",
    "                if len(neighbors) > 20:\n",
    "                    neighbors = np.random.choice(neighbors, 20, replace=False)\n",
    "                neighbors = list(neighbors) + [tx_id]\n",
    "                subgraph = G.subgraph(neighbors)\n",
    "        \n",
    "        # Visualize the subgraph\n",
    "        title = f\"Transaction Network: Fraud Cluster {cluster_id}\"\n",
    "        output_path = f\"../reports/case_study/cluster_{cluster_id}_network.png\"\n",
    "        visualize_transaction_network(subgraph, central_node=tx_id, \n",
    "                                      title=title, output_path=output_path)\n",
    "        \n",
    "        # Calculate network statistics\n",
    "        in_degree = G.in_degree(tx_id) if tx_id in G else 0\n",
    "        out_degree = G.out_degree(tx_id) if tx_id in G else 0\n",
    "        \n",
    "        print(f\"Network statistics for transaction {tx_id}:\")\n",
    "        print(f\"In-degree (incoming transactions): {in_degree}\")\n",
    "        print(f\"Out-degree (outgoing transactions): {out_degree}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing transaction {tx_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time-based Analysis of Fraud Patterns\n",
    "\n",
    "If time step information is available, let's analyze how fraud patterns evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if time step information is available\n",
    "if 'time_step' in results_df.columns:\n",
    "    # Add time step information to fraud_plot_df\n",
    "    fraud_plot_df = fraud_plot_df.merge(\n",
    "        results_df[['tx_id', 'time_step']], \n",
    "        on='tx_id', how='left'\n",
    "    )\n",
    "    \n",
    "    # Analyze fraud clusters by time step\n",
    "    time_cluster_counts = fraud_plot_df.groupby(['time_step', 'cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Plot the evolution of clusters over time\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    time_cluster_counts.plot(kind='bar', stacked=True, cmap='tab10')\n",
    "    plt.title('Evolution of Fraud Clusters Over Time', fontsize=15)\n",
    "    plt.xlabel('Time Step', fontsize=12)\n",
    "    plt.ylabel('Number of Fraudulent Transactions', fontsize=12)\n",
    "    plt.legend(title='Cluster', fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/case_study/fraud_evolution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate cluster proportions by time step\n",
    "    time_cluster_props = time_cluster_counts.div(time_cluster_counts.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Plot the proportions\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    time_cluster_props.plot(kind='bar', stacked=True, cmap='tab10')\n",
    "    plt.title('Proportion of Fraud Clusters Over Time', fontsize=15)\n",
    "    plt.xlabel('Time Step', fontsize=12)\n",
    "    plt.ylabel('Proportion of Fraudulent Transactions', fontsize=12)\n",
    "    plt.legend(title='Cluster', fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/case_study/fraud_proportions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Time step information not available. Skipping time-based analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis of False Negatives\n",
    "\n",
    "Let's analyze fraudulent transactions that the model failed to detect (false negatives) to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract false negatives (fraudulent transactions classified as legitimate)\n",
    "false_negatives = results_df[(results_df['true_label'] == 1) & (results_df['predicted_label'] == 0)]\n",
    "print(f\"Analyzing {len(false_negatives)} false negatives (missed fraud)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of false negatives by cluster\n",
    "fn_plot_df = fraud_plot_df[fraud_plot_df['correct_prediction'] == False]\n",
    "fn_cluster_counts = fn_plot_df.groupby('cluster').size()\n",
    "fn_cluster_props = (fn_plot_df.groupby('cluster').size() / \n",
    "                    fraud_plot_df.groupby('cluster').size())\n",
    "\n",
    "# Plot the false negative analysis by cluster\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Number of false negatives by cluster\n",
    "fn_cluster_counts.plot(kind='bar', ax=ax1, color='orange')\n",
    "ax1.set_title('False Negatives by Cluster', fontsize=14)\n",
    "ax1.set_xlabel('Cluster', fontsize=12)\n",
    "ax1.set_ylabel('Number of False Negatives', fontsize=12)\n",
    "\n",
    "# Proportion of false negatives by cluster\n",
    "fn_cluster_props.plot(kind='bar', ax=ax2, color='red')\n",
    "ax2.set_title('False Negative Rate by Cluster', fontsize=14)\n",
    "ax2.set_xlabel('Cluster', fontsize=12)\n",
    "ax2.set_ylabel('False Negative Rate', fontsize=12)\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/case_study/false_negatives_by_cluster.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a sample of false negatives\n",
    "if len(false_negatives) > 0:\n",
    "    # Select a few false negative examples\n",
    "    num_examples = min(3, len(false_negatives))\n",
    "    fn_examples = false_negatives.sample(num_examples, random_state=42)\n",
    "    \n",
    "    for i, (_, row) in enumerate(fn_examples.iterrows()):\n",
    "        print(f\"\\nFalse Negative Example {i+1}:\")\n",
    "        print(f\"Transaction ID: {row['tx_id']}\")\n",
    "        print(f\"Fraud Probability: {row['fraud_probability']:.4f}\")\n",
    "        \n",
    "        # Extract and visualize the transaction's neighborhood\n",
    "        try:\n",
    "            subgraph = extract_transaction_neighborhood(G, row['tx_id'], depth=1)\n",
    "            print(f\"Extracted subgraph with {subgraph.number_of_nodes()} nodes and {subgraph.number_of_edges()} edges\")\n",
    "            \n",
    "            # Limit to smaller graphs for visualization\n",
    "            if subgraph.number_of_nodes() > 50:\n",
    "                print(f\"Subgraph is too large. Sampling a smaller neighborhood.\")\n",
    "                subgraph = extract_transaction_neighborhood(G, row['tx_id'], depth=0)\n",
    "                if subgraph.number_of_nodes() > 50:\n",
    "                    # If still too large, just show immediate neighbors\n",
    "                    neighbors = list(G.predecessors(row['tx_id'])) + list(G.successors(row['tx_id']))\n",
    "                    if len(neighbors) > 20:\n",
    "                        neighbors = np.random.choice(neighbors, 20, replace=False)\n",
    "                    neighbors = list(neighbors) + [row['tx_id']]\n",
    "                    subgraph = G.subgraph(neighbors)\n",
    "            \n",
    "            # Calculate network statistics\n",
    "            in_degree = G.in_degree(row['tx_id']) if row['tx_id'] in G else 0\n",
    "            out_degree = G.out_degree(row['tx_id']) if row['tx_id'] in G else 0\n",
    "            \n",
    "            print(f\"Network statistics:\")\n",
    "            print(f\"In-degree (incoming transactions): {in_degree}\")\n",
    "            print(f\"Out-degree (outgoing transactions): {out_degree}\")\n",
    "            \n",
    "            # Visualize the subgraph\n",
    "            title = f\"False Negative Example {i+1}: Transaction {row['tx_id']}\"\n",
    "            output_path = f\"../reports/case_study/false_negative_{i+1}.png\"\n",
    "            visualize_transaction_network(subgraph, central_node=row['tx_id'], \n",
    "                                         title=title, output_path=output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing transaction {row['tx_id']}: {str(e)}\")\n",
    "else:\n",
    "    print(\"No false negatives to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance for Different Fraud Patterns\n",
    "\n",
    "Let's analyze which features are most important for detecting each type of fraud pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the model's last layer\n",
    "with torch.no_grad():\n",
    "    # Get the weights from the last layer\n",
    "    last_layer_weights = model.convs[-1].lin.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # Extract weights for the fraud class (index 1)\n",
    "    fraud_weights = last_layer_weights[1, :]\n",
    "    \n",
    "# Create a feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature_index': np.arange(len(fraud_weights)),\n",
    "    'feature_name': feature_names[:len(fraud_weights)] if len(feature_names) >= len(fraud_weights) else [f\"Feature_{i}\" for i in range(len(fraud_weights))],\n",
    "    'importance': np.abs(fraud_weights),\n",
    "    'raw_weight': fraud_weights\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 20 most important features\n",
    "print(\"Top 20 Features for Fraud Detection:\")\n",
    "feature_importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for each cluster\n",
    "# For each cluster, we'll calculate the average feature values and compare to overall\n",
    "\n",
    "# Get feature values for fraudulent transactions\n",
    "fraud_features = data.x[fraud_idx].cpu().numpy()\n",
    "\n",
    "# Calculate overall mean feature values for fraudulent transactions\n",
    "overall_fraud_means = fraud_features.mean(axis=0)\n",
    "\n",
    "# Create a DataFrame to store feature statistics by cluster\n",
    "cluster_feature_stats = pd.DataFrame()\n",
    "\n",
    "# Calculate feature means for each cluster\n",
    "for cluster_id in np.sort(fraud_plot_df['cluster'].unique()):\n",
    "    # Get indices of transactions in this cluster\n",
    "    cluster_tx_ids = fraud_plot_df[fraud_plot_df['cluster'] == cluster_id]['tx_id'].values\n",
    "    \n",
    "    # Map transaction IDs to indices in the data\n",
    "    id_to_idx = dict(zip(df_nodes.iloc[:, 0].values, range(len(df_nodes))))\n",
    "    cluster_indices = [id_to_idx[tx_id] for tx_id in cluster_tx_ids if tx_id in id_to_idx]\n",
    "    \n",
    "    if len(cluster_indices) > 0:\n",
    "        # Get feature values for this cluster\n",
    "        cluster_features = data.x[cluster_indices].cpu().numpy()\n",
    "        \n",
    "        # Calculate mean feature values\n",
    "        cluster_means = cluster_features.mean(axis=0)\n",
    "        \n",
    "        # Calculate relative difference from overall fraud means\n",
    "        relative_diff = (cluster_means - overall_fraud_means) / (overall_fraud_means + 1e-10)  # Avoid division by zero\n",
    "        \n",
    "        # Add to DataFrame\n",
    "        cluster_feature_stats[f'Cluster_{cluster_id}_Mean'] = cluster_means\n",
    "        cluster_feature_stats[f'Cluster_{cluster_id}_RelDiff'] = relative_diff\n",
    "    \n",
    "# Add overall means and feature names\n",
    "cluster_feature_stats['Overall_Fraud_Mean'] = overall_fraud_means\n",
    "cluster_feature_stats['Feature_Index'] = np.arange(len(overall_fraud_means))\n",
    "cluster_feature_stats['Feature_Name'] = feature_names[:len(overall_fraud_means)] if len(feature_names) >= len(overall_fraud_means) else [f\"Feature_{i}\" for i in range(len(overall_fraud_means))]\n",
    "cluster_feature_stats['Importance'] = feature_importance_df['importance'].values\n",
    "\n",
    "# Sort by overall importance\n",
    "cluster_feature_stats = cluster_feature_stats.sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display distinctive features for each cluster\n",
    "top_n = 10  # Number of top features to display\n",
    "\n",
    "for cluster_id in np.sort(fraud_plot_df['cluster'].unique()):\n",
    "    if cluster_id == -1:  # Skip noise\n",
    "        continue\n",
    "        \n",
    "    rel_diff_col = f'Cluster_{cluster_id}_RelDiff'\n",
    "    if rel_diff_col in cluster_feature_stats.columns:\n",
    "        # Sort by absolute relative difference for this cluster\n",
    "        cluster_distinctive = cluster_feature_stats.copy()\n",
    "        cluster_distinctive['Abs_RelDiff'] = np.abs(cluster_distinctive[rel_diff_col])\n",
    "        cluster_distinctive = cluster_distinctive.sort_values('Abs_RelDiff', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Distinctive Features for Cluster {cluster_id}:\")\n",
    "        for i, (_, row) in enumerate(cluster_distinctive.head(top_n).iterrows()):\n",
    "            rel_diff = row[rel_diff_col]\n",
    "            direction = \"higher\" if rel_diff > 0 else \"lower\"\n",
    "            print(f\"{i+1}. {row['Feature_Name']}: {abs(rel_diff):.2f} times {direction} than average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distinctive features for each cluster\n",
    "for cluster_id in np.sort(fraud_plot_df['cluster'].unique()):\n",
    "    if cluster_id == -1:  # Skip noise\n",
    "        continue\n",
    "        \n",
    "    rel_diff_col = f'Cluster_{cluster_id}_RelDiff'\n",
    "    if rel_diff_col in cluster_feature_stats.columns:\n",
    "        # Get top distinctive features\n",
    "        cluster_distinctive = cluster_feature_stats.copy()\n",
    "        cluster_distinctive['Abs_RelDiff'] = np.abs(cluster_distinctive[rel_diff_col])\n",
    "        cluster_distinctive = cluster_distinctive.sort_values('Abs_RelDiff', ascending=False).head(10)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(range(len(cluster_distinctive)), \n",
    "                        cluster_distinctive[rel_diff_col], \n",
    "                        color=[plt.cm.RdBu(0.1) if x < 0 else plt.cm.RdBu(0.9) for x in cluster_distinctive[rel_diff_col]])\n",
    "        plt.yticks(range(len(cluster_distinctive)), \n",
    "                  [f\"{name} (Imp: {imp:.3f})\" for name, imp in \n",
    "                   zip(cluster_distinctive['Feature_Name'], cluster_distinctive['Importance'])])\n",
    "        plt.axvline(x=0, color='black', linestyle='--')\n",
    "        plt.title(f\"Distinctive Features for Fraud Cluster {cluster_id}\", fontsize=15)\n",
    "        plt.xlabel(\"Relative Difference from Average (+ = higher, - = lower)\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../reports/case_study/cluster_{cluster_id}_features.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights\n",
    "\n",
    "Based on our analysis of the blockchain fraud patterns, we can draw the following insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "summary_stats = {\n",
    "    'total_transactions': len(results_df),\n",
    "    'fraud_transactions': results_df['true_label'].sum(),\n",
    "    'fraud_rate': results_df['true_label'].mean(),\n",
    "    'detection_rate': results_df[results_df['true_label'] == 1]['correct_prediction'].mean(),\n",
    "    'false_positive_rate': len(results_df[(results_df['true_label'] == 0) & (results_df['predicted_label'] == 1)]) / len(results_df[results_df['true_label'] == 0]),\n",
    "    'clusters_identified': n_clusters,\n",
    "    'top_features': feature_importance_df['feature_name'].head(5).tolist()\n",
    "}\n",
    "\n",
    "print(\"Blockchain Fraud Analysis Summary:\")\n",
    "print(f\"Total Transactions: {summary_stats['total_transactions']}\")\n",
    "print(f\"Fraudulent Transactions: {summary_stats['fraud_transactions']} ({summary_stats['fraud_rate']:.2%})\")\n",
    "print(f\"Fraud Detection Rate: {summary_stats['detection_rate']:.2%}\")\n",
    "print(f\"False Positive Rate: {summary_stats['false_positive_rate']:.2%}\")\n",
    "print(f\"Fraud Patterns (Clusters) Identified: {summary_stats['clusters_identified']}\")\n",
    "print(f\"Top 5 Important Features: {', '.join(summary_stats['top_features'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a markdown summary report\n",
    "summary_md = f\"\"\"# Blockchain Fraud Case Study: Patterns and Insights\n",
    "\n",
    "## Summary Statistics\n",
    "- **Total Transactions:** {summary_stats['total_transactions']}\n",
    "- **Fraudulent Transactions:** {summary_stats['fraud_transactions']} ({summary_stats['fraud_rate']:.2%})\n",
    "- **Fraud Detection Rate:** {summary_stats['detection_rate']:.2%}\n",
    "- **False Positive Rate:** {summary_stats['false_positive_rate']:.2%}\n",
    "- **Distinct Fraud Patterns:** {summary_stats['clusters_identified']} clusters identified\n",
    "\n",
    "## Key Fraud Indicators\n",
    "The most important features for detecting fraud are:\n",
    "{', '.join([f'**{f}**' for f in summary_stats['top_features']])}\n",
    "\n",
    "## Fraud Pattern Analysis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Add information about each cluster\n",
    "for cluster_id in sorted(cluster_stats['Cluster'].unique()):\n",
    "    if cluster_id == -1:  # Skip noise in the summary\n",
    "        continue\n",
    "        \n",
    "    cluster_count = cluster_stats[cluster_stats['Cluster'] == cluster_id]['Count'].iloc[0]\n",
    "    detection_rate = cluster_stats[cluster_stats['Cluster'] == cluster_id]['Detection_Rate'].iloc[0]\n",
    "    \n",
    "    summary_md += f\"\"\"### Fraud Pattern {cluster_id}\n",
    "- **Transactions:** {cluster_count} ({cluster_count / summary_stats['fraud_transactions']:.2%} of all fraud)\n",
    "- **Detection Rate:** {detection_rate:.2%}\n",
    "- **Key Characteristics:**\n",
    "\"\"\"\n",
    "    \n",
    "    # Add distinctive features for this cluster\n",
    "    rel_diff_col = f'Cluster_{cluster_id}_RelDiff'\n",
    "    if rel_diff_col in cluster_feature_stats.columns:\n",
    "        cluster_distinctive = cluster_feature_stats.copy()\n",
    "        cluster_distinctive['Abs_RelDiff'] = np.abs(cluster_distinctive[rel_diff_col])\n",
    "        cluster_distinctive = cluster_distinctive.sort_values('Abs_RelDiff', ascending=False).head(5)\n",
    "        \n",
    "        for _, row in cluster_distinctive.iterrows():\n",
    "            rel_diff = row[rel_diff_col]\n",
    "            direction = \"higher\" if rel_diff > 0 else \"lower\"\n",
    "            summary_md += f\"  - {row['Feature_Name']}: {abs(rel_diff):.2f} times {direction} than average\\n\"\n",
    "    \n",
    "    summary_md += \"\\n\"\n",
    "\n",
    "# Add conclusions\n",
    "summary_md += \"\"\"## Conclusions and Recommendations\n",
    "\n",
    "The analysis reveals distinct patterns of fraudulent behavior in the blockchain transaction network. Key insights include:\n",
    "\n",
    "1. **Multiple Fraud Patterns:** We identified several distinct clusters of fraudulent transactions, each with unique characteristics.\n",
    "\n",
    "2. **Network Structure:** The network analysis shows that fraudulent transactions often have distinctive connection patterns in the transaction graph.\n",
    "\n",
    "3. **Feature Importance:** Certain features are consistently important across fraud patterns, while others are specific to particular types of fraud.\n",
    "\n",
    "4. **Detection Challenges:** Some fraud patterns are more challenging to detect than others, with varying detection rates across clusters.\n",
    "\n",
    "### Recommendations for Improving Fraud Detection:\n",
    "\n",
    "1. **Enhanced Feature Engineering:** Develop specialized features for each fraud pattern to improve detection across all types.\n",
    "\n",
    "2. **Pattern-Specific Models:** Consider training separate models for different fraud patterns or using ensemble methods.\n",
    "\n",
    "3. **Graph Features:** Further leverage the transaction network structure, as graph-based features are highly informative.\n",
    "\n",
    "4. **Continuous Monitoring:** Implement real-time monitoring for emerging fraud patterns, as they may evolve over time.\n",
    "\"\"\"\n",
    "\n",
    "# Save the summary report\n",
    "with open('../reports/case_study/summary.md', 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(\"Saved summary report to '../reports/case_study/summary.md'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we conducted a detailed case study of blockchain fraud patterns using our trained GNN model. We identified distinct clusters of fraudulent transactions, analyzed their network structures, and examined the features that best characterize each fraud pattern.\n",
    "\n",
    "Our analysis revealed that:\n",
    "1. Fraudulent transactions can be grouped into several distinct patterns, each with unique characteristics\n",
    "2. Different fraud patterns have varying detection rates, with some being more challenging to identify than others\n",
    "3. Network structure and connection patterns provide valuable insights into fraudulent behavior\n",
    "4. Certain features are consistently important across all fraud types, while others are specific to particular patterns\n",
    "5. The combination of transaction features and graph structure is powerful for fraud detection\n",
    "\n",
    "These insights can guide the development of more specialized fraud detection strategies and inform future feature engineering efforts. By understanding the distinctive characteristics of different fraud patterns, we can build more effective models that target specific types of fraudulent activity in blockchain networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
