{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Transaction Fraud Detection: Data Preparation\n",
    "\n",
    "This notebook prepares the Bitcoin transaction dataset for fraud detection using Graph Neural Networks (GNNs). We'll load the transaction data, preprocess it, and prepare it for training GNN models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Loading Data](#Loading-Data)\n",
    "3. [Data Exploration](#Data-Exploration)\n",
    "4. [Data Preprocessing](#Data-Preprocessing)\n",
    "5. [Creating Graph Representation](#Creating-Graph-Representation)\n",
    "6. [Data Splitting](#Data-Splitting)\n",
    "7. [Saving Processed Data](#Saving-Processed-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create directories for data\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The blockchain dataset consists of three main files:\n",
    "1. `classes.csv` - Contains transaction IDs and their classes (legitimate or fraudulent)\n",
    "2. `edgelist.csv` - Contains transaction connections (edges)\n",
    "3. `Features.csv` - Contains transaction features\n",
    "\n",
    "Let's load these files and examine their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(classes_path, edgelist_path, features_path):\n",
    "    \"\"\"\n",
    "    Load blockchain dataset from CSV files with the special format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classes_path : str\n",
    "        Path to the classes CSV file with txId and class columns\n",
    "    edgelist_path : str\n",
    "        Path to the edgelist CSV file with txId1 and txId2 columns\n",
    "    features_path : str\n",
    "        Path to the features CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data with txId, class, and features\n",
    "    df_edges : pandas.DataFrame\n",
    "        DataFrame containing edge data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from {classes_path}, {edgelist_path}, and {features_path}\")\n",
    "    \n",
    "    # Load classes data\n",
    "    df_classes = pd.read_csv(classes_path)\n",
    "    logger.info(f\"Loaded {len(df_classes)} transactions with class information\")\n",
    "    \n",
    "    # Load edge data\n",
    "    df_edges = pd.read_csv(edgelist_path)\n",
    "    logger.info(f\"Loaded {len(df_edges)} edges\")\n",
    "    \n",
    "    # Load features data - this has a non-standard format\n",
    "    try:\n",
    "        # Read the features file\n",
    "        df_features = pd.read_csv(features_path)\n",
    "        \n",
    "        # First column should be txId\n",
    "        df_features = df_features.rename(columns={df_features.columns[0]: 'txId'})\n",
    "        \n",
    "        # Second column is a constant (1) - drop it\n",
    "        df_features = df_features.drop(columns=[df_features.columns[1]])\n",
    "        \n",
    "        # Rename remaining columns to feature_0, feature_1, etc.\n",
    "        feature_cols = [col for col in df_features.columns if col != 'txId']\n",
    "        feature_rename = {col: f'feature_{i}' for i, col in enumerate(feature_cols)}\n",
    "        df_features = df_features.rename(columns=feature_rename)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_features.columns)-1} features for {len(df_features)} transactions\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading features: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Merge classes and features based on txId\n",
    "    df_nodes = pd.merge(df_classes, df_features, on='txId', how='inner')\n",
    "    logger.info(f\"Combined data has {len(df_nodes)} transactions with both class and feature information\")\n",
    "    \n",
    "    return df_nodes, df_edges\n",
    "\n",
    "# Paths to data files\n",
    "classes_path = 'data/raw/classes.csv'\n",
    "edgelist_path = 'data/raw/edgelist.csv'\n",
    "features_path = 'data/raw/Features.csv'\n",
    "\n",
    "# Check if files exist\n",
    "files_exist = all(os.path.exists(p) for p in [classes_path, edgelist_path, features_path])\n",
    "\n",
    "if files_exist:\n",
    "    # Load data\n",
    "    df_nodes, df_edges = load_data(classes_path, edgelist_path, features_path)\n",
    "else:\n",
    "    logger.error(\"Required input files not found. Please place them in the data/raw directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the loaded data to better understand its structure and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the node data\n",
    "print(\"Node data (first 5 rows):\")\n",
    "df_nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the edge data\n",
    "print(\"Edge data (first 5 rows):\")\n",
    "df_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics about the dataset\n",
    "print(\"Dataset statistics:\")\n",
    "print(f\"Number of transactions: {len(df_nodes)}\")\n",
    "print(f\"Number of edges: {len(df_edges)}\")\n",
    "print(f\"Number of features per transaction: {len(df_nodes.columns) - 2}\")\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = df_nodes['class'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"Percentage fraudulent: {class_counts.get(1, 0) / len(df_nodes) * 100:.2f}%\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='class', data=df_nodes, palette='viridis')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0: Legitimate, 1: Fraudulent)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():,}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature distributions\n",
    "feature_cols = [col for col in df_nodes.columns if col not in ['txId', 'class']]\n",
    "\n",
    "# Calculate feature statistics\n",
    "feature_stats = df_nodes[feature_cols].describe().T\n",
    "feature_stats['variance'] = df_nodes[feature_cols].var()\n",
    "feature_stats = feature_stats.sort_values('variance', ascending=False)\n",
    "\n",
    "# Display the top 10 features with highest variance\n",
    "print(\"Top 10 features with highest variance:\")\n",
    "feature_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of a few high-variance features\n",
    "top_features = feature_stats.index[:4]  # Top 4 high-variance features\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    sns.histplot(df_nodes[feature], bins=50, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore feature correlations with the target class\n",
    "correlations = []\n",
    "for feature in feature_cols:\n",
    "    corr = df_nodes[feature].corr(df_nodes['class'])\n",
    "    correlations.append((feature, corr))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display top 15 correlated features\n",
    "print(\"Top 15 features by correlation with the target:\")\n",
    "pd.DataFrame(correlations[:15], columns=['Feature', 'Correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small network visualization to understand the graph structure\n",
    "# We'll take a sample for visualization purposes\n",
    "sample_size = min(1000, len(df_nodes))  # Limit to 1000 nodes for visualization\n",
    "sampled_nodes = df_nodes.sample(sample_size, random_state=42)\n",
    "\n",
    "# Create a graph with sampled nodes\n",
    "G = nx.DiGraph()\n",
    "for _, node in sampled_nodes.iterrows():\n",
    "    G.add_node(node['txId'], fraud=(node['class'] == 1))\n",
    "\n",
    "# Add edges connecting the sampled nodes\n",
    "for _, edge in df_edges.iterrows():\n",
    "    if edge['txId1'] in G.nodes and edge['txId2'] in G.nodes:\n",
    "        G.add_edge(edge['txId1'], edge['txId2'])\n",
    "\n",
    "# Set node colors based on class (red for fraud, blue for legitimate)\n",
    "node_colors = ['red' if G.nodes[node]['fraud'] else 'blue' for node in G.nodes]\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, seed=42)  # Position nodes using force-directed layout\n",
    "nx.draw_networkx(G, pos, with_labels=False, node_size=50, node_color=node_colors, alpha=0.7, arrows=False)\n",
    "plt.title(f'Sample Transaction Network (Red = Fraudulent, Blue = Legitimate)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about the sampled graph\n",
    "print(f\"Sampled graph statistics:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now that we've explored the data, let's preprocess it for training GNN models. This includes handling unknown classes, mapping non-numeric class labels to integers, removing low-variance features, and normalizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_mapping(df_nodes):\n",
    "    \"\"\"\n",
    "    Create a mapping between transaction IDs and indices for graph construction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    id2idx : dict\n",
    "        Dictionary mapping transaction IDs to indices\n",
    "    \"\"\"\n",
    "    # Create mapping of txId to index\n",
    "    id2idx = {tx_id: i for i, tx_id in enumerate(df_nodes['txId'])}\n",
    "    logger.info(f\"Created mapping for {len(id2idx)} transactions\")\n",
    "    return id2idx\n",
    "\n",
    "def preprocess_data(df_nodes, df_edges):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by handling unknown classes and normalizing features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data\n",
    "    df_edges : pandas.DataFrame\n",
    "        DataFrame containing edge data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_processed : pandas.DataFrame\n",
    "        Processed DataFrame\n",
    "    df_edges : pandas.DataFrame\n",
    "        Processed edges DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Preprocessing data\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_processed = df_nodes.copy()\n",
    "    \n",
    "    # Handle unknown classes\n",
    "    unknown_mask = df_processed['class'] == 'unknown'\n",
    "    unknown_count = unknown_mask.sum()\n",
    "    logger.info(f\"Found {unknown_count} transactions with unknown class ({unknown_count/len(df_processed):.2%})\")\n",
    "    \n",
    "    if unknown_count > 0:\n",
    "        df_processed = df_processed[~unknown_mask]\n",
    "        logger.info(f\"Removed transactions with unknown class. Remaining: {len(df_processed)}\")\n",
    "    \n",
    "    # Convert non-numeric class labels to integers\n",
    "    if df_processed['class'].dtype == 'object':\n",
    "        unique_classes = df_processed['class'].unique()\n",
    "        class_map = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "        df_processed['class'] = df_processed['class'].map(class_map)\n",
    "        logger.info(f\"Mapped class values to integers: {class_map}\")\n",
    "    \n",
    "    # Identify feature columns (exclude txId and class)\n",
    "    feature_cols = [col for col in df_processed.columns if col not in ['txId', 'class']]\n",
    "    \n",
    "    # Check for and remove low-variance features\n",
    "    variance = df_processed[feature_cols].var()\n",
    "    low_var_threshold = 0.01\n",
    "    low_var_cols = variance[variance < low_var_threshold].index.tolist()\n",
    "    \n",
    "    if low_var_cols:\n",
    "        logger.info(f\"Removing {len(low_var_cols)} low-variance features\")\n",
    "        df_processed = df_processed.drop(columns=low_var_cols)\n",
    "        # Update feature columns\n",
    "        feature_cols = [col for col in df_processed.columns if col not in ['txId', 'class']]\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    df_processed[feature_cols] = scaler.fit_transform(df_processed[feature_cols])\n",
    "    \n",
    "    # Make sure edges only include transactions with known classes\n",
    "    valid_tx_ids = set(df_processed['txId'])\n",
    "    edges_before = len(df_edges)\n",
    "    df_edges = df_edges[df_edges['txId1'].isin(valid_tx_ids) & df_edges['txId2'].isin(valid_tx_ids)]\n",
    "    edges_after = len(df_edges)\n",
    "    logger.info(f\"Filtered edges to include only known transactions: {edges_before} -> {edges_after}\")\n",
    "    \n",
    "    return df_processed, df_edges\n",
    "\n",
    "# Preprocess the data\n",
    "df_processed, df_edges = preprocess_data(df_nodes, df_edges)\n",
    "\n",
    "# Create node mapping\n",
    "id2idx = create_node_mapping(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Graph Representation\n",
    "\n",
    "Now we'll create the graph representation for PyTorch Geometric, which includes the edge index tensor, node features, and node labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(df_edges, id2idx):\n",
    "    \"\"\"\n",
    "    Construct edge_index tensor for PyTorch Geometric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_edges : pandas.DataFrame\n",
    "        DataFrame containing edge data\n",
    "    id2idx : dict\n",
    "        Dictionary mapping transaction IDs to indices\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    edge_index : torch.LongTensor\n",
    "        Edge index tensor for PyTorch Geometric\n",
    "    \"\"\"\n",
    "    logger.info(\"Building edge index tensor\")\n",
    "    \n",
    "    edges = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for _, row in df_edges.iterrows():\n",
    "        source_id, target_id = row['txId1'], row['txId2']\n",
    "        \n",
    "        # Check if both nodes exist in the mapping\n",
    "        if source_id in id2idx and target_id in id2idx:\n",
    "            source_idx = id2idx[source_id]\n",
    "            target_idx = id2idx[target_id]\n",
    "            edges.append([source_idx, target_idx])\n",
    "        else:\n",
    "            skipped += 1\n",
    "    \n",
    "    if skipped > 0:\n",
    "        logger.warning(f\"Skipped {skipped} edges with missing transaction IDs\")\n",
    "    \n",
    "    if not edges:\n",
    "        logger.warning(\"No valid edges found\")\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        # Convert to torch tensor\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    logger.info(f\"Built edge index with shape {edge_index.shape}\")\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "# Build edge index tensor\n",
    "edge_index = build_edge_index(df_edges, id2idx)\n",
    "\n",
    "# Print statistics about the edge index\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Number of edges: {edge_index.shape[1]}\")\n",
    "print(f\"Number of unique source nodes: {len(set(edge_index[0].numpy()))}\")\n",
    "print(f\"Number of unique target nodes: {len(set(edge_index[1].numpy()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Now we'll split the data into training, validation, and test sets. We'll use stratified sampling to ensure that each split has the same class distribution as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_splits(df_processed, edge_index, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits and PyTorch Geometric Data object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_processed : pandas.DataFrame\n",
    "        Processed DataFrame\n",
    "    edge_index : torch.LongTensor\n",
    "        Edge index tensor\n",
    "    train_size, val_size, test_size : float\n",
    "        Proportions for train/val/test splits\n",
    "    random_state : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data : torch_geometric.data.Data\n",
    "        PyTorch Geometric Data object\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating data splits\")\n",
    "    \n",
    "    # Get feature matrix\n",
    "    feature_cols = [col for col in df_processed.columns if col not in ['txId', 'class']]\n",
    "    features = torch.FloatTensor(df_processed[feature_cols].values)\n",
    "    \n",
    "    # Get labels\n",
    "    labels = torch.LongTensor(df_processed['class'].values)\n",
    "    \n",
    "    # Create Data object\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels)\n",
    "    \n",
    "    # Create splits\n",
    "    indices = np.arange(len(df_processed))\n",
    "    \n",
    "    try:\n",
    "        # First split: train vs. (val+test)\n",
    "        train_idx, temp_idx = train_test_split(\n",
    "            indices, \n",
    "            train_size=train_size, \n",
    "            stratify=df_processed['class'].values,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Second split: val vs. test\n",
    "        val_size_adjusted = val_size / (val_size + test_size)\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            temp_idx,\n",
    "            train_size=val_size_adjusted,\n",
    "            stratify=df_processed.iloc[temp_idx]['class'].values,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # If stratified split fails (e.g., too few samples in some class),\n",
    "        # fall back to regular split\n",
    "        logger.warning(f\"Stratified split failed: {str(e)}. Using random split.\")\n",
    "        \n",
    "        train_idx, temp_idx = train_test_split(\n",
    "            indices, \n",
    "            train_size=train_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        val_size_adjusted = val_size / (val_size + test_size)\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            temp_idx,\n",
    "            train_size=val_size_adjusted,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    \n",
    "    # Create split dictionary\n",
    "    split_idx = {\n",
    "        'train': train_idx,\n",
    "        'val': val_idx,\n",
    "        'test': test_idx\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Created splits: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "    \n",
    "    return data, split_idx\n",
    "\n",
    "# Create data splits and PyTorch Geometric Data object\n",
    "data, split_idx = create_data_splits(df_processed, edge_index)\n",
    "\n",
    "# Print information about the data object\n",
    "print(\"PyTorch Geometric Data object:\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {len(torch.unique(data.y))}\")\n",
    "\n",
    "# Print information about the splits\n",
    "print(\"\\nData splits:\")\n",
    "print(f\"Training set: {len(split_idx['train'])} nodes ({len(split_idx['train'])/data.num_nodes:.2%})\")\n",
    "print(f\"Validation set: {len(split_idx['val'])} nodes ({len(split_idx['val'])/data.num_nodes:.2%})\")\n",
    "print(f\"Test set: {len(split_idx['test'])} nodes ({len(split_idx['test'])/data.num_nodes:.2%})\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "train_class_dist = np.bincount(data.y[split_idx['train']].numpy())\n",
    "val_class_dist = np.bincount(data.y[split_idx['val']].numpy())\n",
    "test_class_dist = np.bincount(data.y[split_idx['test']].numpy())\n",
    "\n",
    "print(\"\\nClass distribution in each split:\")\n",
    "print(f\"Training set: {train_class_dist} ({train_class_dist / train_class_dist.sum() * 100}%)\")\n",
    "print(f\"Validation set: {val_class_dist} ({val_class_dist / val_class_dist.sum() * 100}%)\")\n",
    "print(f\"Test set: {test_class_dist} ({test_class_dist / test_class_dist.sum() * 100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Processed Data\n",
    "\n",
    "Finally, let's save the processed data and splits for later use in training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(data, split_idx, output_dir='data/processed'):\n",
    "    \"\"\"\n",
    "    Save processed data and splits to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : torch_geometric.data.Data\n",
    "        PyTorch Geometric Data object\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    output_dir : str\n",
    "        Directory to save processed data\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    logger.info(f\"Saving processed data to {output_dir}\")\n",
    "    \n",
    "    # Save data object\n",
    "    torch.save(data, os.path.join(output_dir, 'data.pt'))\n",
    "    \n",
    "    # Save edge index separately\n",
    "    torch.save(data.edge_index, os.path.join(output_dir, 'edge_index.pt'))\n",
    "    \n",
    "    # Save features and labels\n",
    "    np.save(os.path.join(output_dir, 'features.npy'), data.x.numpy())\n",
    "    np.save(os.path.join(output_dir, 'labels.npy'), data.y.numpy())\n",
    "    \n",
    "    # Save splits\n",
    "    for split in split_idx:\n",
    "        np.save(os.path.join(output_dir, f'{split}_idx.npy'), split_idx[split])\n",
    "    \n",
    "    logger.info(f\"Successfully saved processed data to {output_dir}\")\n",
    "\n",
    "# Save processed data\n",
    "save_processed_data(data, split_idx)\n",
    "\n",
    "print(\"Data preparation completed successfully!\")\n",
    "print(\"Saved processed data to data/processed/ directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. Loaded the Bitcoin transaction dataset (transactions, features, and edges)\n",
    "2. Explored the data to understand its structure and characteristics\n",
    "3. Preprocessed the data by handling unknown classes, removing low-variance features, and normalizing features\n",
    "4. Created a graph representation using PyTorch Geometric\n",
    "5. Split the data into training, validation, and test sets\n",
    "6. Saved the processed data for later use\n",
    "\n",
    "The processed data is now ready for feature engineering and model training in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
