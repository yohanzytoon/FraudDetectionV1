{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Transaction Fraud Detection: Model Training\n",
    "\n",
    "This notebook focuses on implementing and training Graph Neural Network (GNN) models for Bitcoin transaction fraud detection. We'll use the processed data and engineered features from the previous notebooks to train several GNN architectures and compare their performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Loading Processed Data](#Loading-Processed-Data)\n",
    "3. [GNN Model Implementation](#GNN-Model-Implementation)\n",
    "   - [Graph Convolutional Network (GCN)](#Graph-Convolutional-Network-GCN)\n",
    "   - [GraphSAGE](#GraphSAGE)\n",
    "   - [Graph Attention Network (GAT)](#Graph-Attention-Network-GAT)\n",
    "4. [Training Functions](#Training-Functions)\n",
    "5. [Model Training](#Model-Training)\n",
    "   - [Training GCN Model](#Training-GCN-Model)\n",
    "   - [Training GraphSAGE Model](#Training-GraphSAGE-Model)\n",
    "   - [Training GAT Model](#Training-GAT-Model)\n",
    "6. [Model Comparison](#Model-Comparison)\n",
    "7. [Saving Models](#Saving-Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Data\n",
    "\n",
    "First, let's load the processed data and engineered features from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(input_dir='data/processed'):\n",
    "    \"\"\"\n",
    "    Load processed data from disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory with processed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data : torch_geometric.data.Data\n",
    "        PyTorch Geometric Data object\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading processed data from {input_dir}\")\n",
    "    \n",
    "    # Load data object\n",
    "    data_path = os.path.join(input_dir, 'data.pt')\n",
    "    \n",
    "    try:\n",
    "        # Try direct loading first\n",
    "        data = torch.load(data_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to load data directly: {e}. Reconstructing from components...\")\n",
    "        # If that fails, try to reconstruct the data object from components\n",
    "        features_path = os.path.join(input_dir, 'features.npy')\n",
    "        labels_path = os.path.join(input_dir, 'labels.npy')\n",
    "        edge_index_path = os.path.join(input_dir, 'edge_index.pt')\n",
    "        \n",
    "        # Check if we have combined features (from feature engineering)\n",
    "        combined_features_path = os.path.join(input_dir, 'combined_features.npy')\n",
    "        if os.path.exists(combined_features_path):\n",
    "            logger.info(\"Loading combined features from feature engineering\")\n",
    "            features = torch.FloatTensor(np.load(combined_features_path))\n",
    "        else:\n",
    "            features = torch.FloatTensor(np.load(features_path))\n",
    "            \n",
    "        labels = torch.LongTensor(np.load(labels_path))\n",
    "        edge_index = torch.load(edge_index_path)\n",
    "        \n",
    "        data = Data(x=features, edge_index=edge_index, y=labels)\n",
    "    \n",
    "    # Load splits\n",
    "    split_idx = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(input_dir, f'{split}_idx.npy')\n",
    "        if os.path.exists(split_path):\n",
    "            split_idx[split] = np.load(split_path)\n",
    "            logger.info(f\"Loaded {split} indices with {len(split_idx[split])} samples\")\n",
    "    \n",
    "    logger.info(f\"Successfully loaded processed data from {input_dir}\")\n",
    "    logger.info(f\"Data contains {data.num_nodes} nodes, {data.num_edges} edges, and {data.num_features} features\")\n",
    "    \n",
    "    return data, split_idx\n",
    "\n",
    "# Load processed data\n",
    "try:\n",
    "    data, split_idx = load_processed_data()\n",
    "    \n",
    "    # Print information about the data\n",
    "    print(\"Data information:\")\n",
    "    print(f\"Number of nodes: {data.num_nodes}\")\n",
    "    print(f\"Number of edges: {data.num_edges}\")\n",
    "    print(f\"Number of features: {data.num_features}\")\n",
    "    print(f\"Number of classes: {len(torch.unique(data.y))}\")\n",
    "    \n",
    "    # Print split sizes\n",
    "    print(\"\\nSplit sizes:\")\n",
    "    for split_name, indices in split_idx.items():\n",
    "        print(f\"{split_name.capitalize()}: {len(indices)} nodes\")\n",
    "        \n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Processed data not found. Please run Data Preparation notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Model Implementation\n",
    "\n",
    "Now, let's implement the GNN models for our task. We'll implement three popular GNN architectures:\n",
    "1. Graph Convolutional Network (GCN)\n",
    "2. GraphSAGE\n",
    "3. Graph Attention Network (GAT)\n",
    "\n",
    "These models capture different aspects of graph structure and have different inductive biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network (GCN)\n",
    "\n",
    "GCN is a popular GNN architecture that performs message passing by aggregating information from neighboring nodes. It's efficient and works well for many graph-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network model for transaction classification.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple GCN layers\n",
    "    - Batch normalization\n",
    "    - Residual connections\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                dropout=0.5, residual=True, batch_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize the GCN model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layers\n",
    "        output_dim : int\n",
    "            Dimension of output (number of classes)\n",
    "        num_layers : int\n",
    "            Number of GCN layers\n",
    "        dropout : float\n",
    "            Dropout probability\n",
    "        residual : bool\n",
    "            Whether to use residual connections\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs = nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([\n",
    "                nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)\n",
    "            ])\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        logger.info(f\"Initialized GCN model with {num_layers} layers\")\n",
    "        logger.info(f\"Input dim: {input_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim}\")\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset all parameters for better initialization\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x : torch.Tensor\n",
    "            Output predictions [num_nodes, output_dim]\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev  # Residual connection\n",
    "                \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, layer=-2):\n",
    "        \"\"\"\n",
    "        Get embeddings from an intermediate layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "        layer : int\n",
    "            Index of the layer to extract embeddings from (negative indices count from end)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        embeddings : torch.Tensor\n",
    "            Node embeddings\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        \n",
    "        # Process up to the desired layer\n",
    "        max_layer = self.num_layers if layer >= 0 else self.num_layers + layer\n",
    "        \n",
    "        for i in range(max_layer):\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if i < self.num_layers - 1:  # Not the last layer\n",
    "                if self.batch_norm and i > 0:\n",
    "                    h = self.bns[i-1](h)\n",
    "                \n",
    "                h = F.relu(h)\n",
    "                \n",
    "                if self.residual and i > 0:\n",
    "                    h_prev = h  # Store for residual connection\n",
    "                    h = h + h_prev\n",
    "                    \n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE\n",
    "\n",
    "GraphSAGE (Sample and Aggregate) is another popular GNN architecture that is designed to scale well to large graphs by sampling a fixed number of neighbors per node during message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE model for transaction classification.\n",
    "    \n",
    "    Features:\n",
    "    - GraphSAGE convolutions\n",
    "    - Batch normalization\n",
    "    - Residual connections\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                dropout=0.5, residual=True, batch_norm=True, aggr='mean'):\n",
    "        \"\"\"\n",
    "        Initialize the GraphSAGE model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layers\n",
    "        output_dim : int\n",
    "            Dimension of output (number of classes)\n",
    "        num_layers : int\n",
    "            Number of SAGE layers\n",
    "        dropout : float\n",
    "            Dropout probability\n",
    "        residual : bool\n",
    "            Whether to use residual connections\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        aggr : str\n",
    "            Aggregation method ('mean', 'max', or 'sum')\n",
    "        \"\"\"\n",
    "        super(SAGEModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs = nn.ModuleList([SAGEConv(input_dim, hidden_dim, aggr=aggr)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim, aggr=aggr))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(SAGEConv(hidden_dim, output_dim, aggr=aggr))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([\n",
    "                nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)\n",
    "            ])\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        logger.info(f\"Initialized GraphSAGE model with {num_layers} layers\")\n",
    "        logger.info(f\"Input dim: {input_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim}\")\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset all parameters for better initialization\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x : torch.Tensor\n",
    "            Output predictions [num_nodes, output_dim]\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev  # Residual connection\n",
    "                \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, layer=-2):\n",
    "        \"\"\"\n",
    "        Get embeddings from an intermediate layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "        layer : int\n",
    "            Index of the layer to extract embeddings from (negative indices count from end)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        embeddings : torch.Tensor\n",
    "            Node embeddings\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        \n",
    "        # Process up to the desired layer\n",
    "        max_layer = self.num_layers if layer >= 0 else self.num_layers + layer\n",
    "        \n",
    "        for i in range(max_layer):\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if i < self.num_layers - 1:  # Not the last layer\n",
    "                if self.batch_norm and i > 0:\n",
    "                    h = self.bns[i-1](h)\n",
    "                \n",
    "                h = F.relu(h)\n",
    "                \n",
    "                if self.residual and i > 0:\n",
    "                    h_prev = h  # Store for residual connection\n",
    "                    h = h + h_prev\n",
    "                    \n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Attention Network (GAT)\n",
    "\n",
    "GAT uses attention mechanisms to weigh the importance of neighboring nodes differently during message passing, which can be especially useful for fraud detection as it can learn to focus on suspicious connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network model for transaction classification.\n",
    "    \n",
    "    Features:\n",
    "    - GAT layers with attention\n",
    "    - Batch normalization\n",
    "    - Residual connections\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                heads=8, dropout=0.5, residual=True, batch_norm=True):\n",
    "        \"\"\"\n",
    "        Initialize the GAT model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input features\n",
    "        hidden_dim : int\n",
    "            Dimension of hidden layers\n",
    "        output_dim : int\n",
    "            Dimension of output (number of classes)\n",
    "        num_layers : int\n",
    "            Number of GAT layers\n",
    "        heads : int\n",
    "            Number of attention heads\n",
    "        dropout : float\n",
    "            Dropout probability\n",
    "        residual : bool\n",
    "            Whether to use residual connections\n",
    "        batch_norm : bool\n",
    "            Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # Input layer (with multiple heads)\n",
    "        self.convs = nn.ModuleList([GATConv(input_dim, hidden_dim // heads, heads=heads)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_dim, hidden_dim // heads, heads=heads))\n",
    "        \n",
    "        # Output layer (with 1 head)\n",
    "        self.convs.append(GATConv(hidden_dim, output_dim, heads=1))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([\n",
    "                nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)\n",
    "            ])\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        logger.info(f\"Initialized GAT model with {num_layers} layers and {heads} heads\")\n",
    "        logger.info(f\"Input dim: {input_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim}\")\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset all parameters for better initialization\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        x : torch.Tensor\n",
    "            Output predictions [num_nodes, output_dim]\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev  # Residual connection\n",
    "                \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, layer=-2):\n",
    "        \"\"\"\n",
    "        Get embeddings from an intermediate layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Node features [num_nodes, input_dim]\n",
    "        edge_index : torch.LongTensor\n",
    "            Graph connectivity [2, num_edges]\n",
    "        layer : int\n",
    "            Index of the layer to extract embeddings from (negative indices count from end)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        embeddings : torch.Tensor\n",
    "            Node embeddings\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        \n",
    "        # Process up to the desired layer\n",
    "        max_layer = self.num_layers if layer >= 0 else self.num_layers + layer\n",
    "        \n",
    "        for i in range(max_layer):\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if i < self.num_layers - 1:  # Not the last layer\n",
    "                if self.batch_norm and i > 0:\n",
    "                    h = self.bns[i-1](h)\n",
    "                \n",
    "                h = F.relu(h)\n",
    "                \n",
    "                if self.residual and i > 0:\n",
    "                    h_prev = h  # Store for residual connection\n",
    "                    h = h + h_prev\n",
    "                    \n",
    "                h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions\n",
    "\n",
    "Let's define a training function that can be used for all GNN models. This function will handle the training loop, early stopping, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, split_idx, optimizer, criterion, \n",
    "               scheduler=None, epochs=200, patience=20, \n",
    "               device='cpu', model_dir='models', model_name='gnn'):\n",
    "    \"\"\"\n",
    "    Train a GNN model with early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The model to train\n",
    "    data : torch_geometric.data.Data\n",
    "        The graph data\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use\n",
    "    criterion : torch.nn.Module\n",
    "        The loss function\n",
    "    scheduler : torch.optim.lr_scheduler._LRScheduler, optional\n",
    "        Learning rate scheduler\n",
    "    epochs : int\n",
    "        Maximum number of epochs\n",
    "    patience : int\n",
    "        Patience for early stopping\n",
    "    device : str\n",
    "        Device to use ('cpu' or 'cuda')\n",
    "    model_dir : str\n",
    "        Directory to save the model\n",
    "    model_name : str\n",
    "        Name of the model for saving\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : torch.nn.Module\n",
    "        The trained model\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Prepare model directory\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_auc': [],\n",
    "        'val_auc': []\n",
    "    }\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    logger.info(f\"Starting training for {epochs} epochs (early stopping patience: {patience})\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        train_loss = criterion(out[split_idx['train']], data.y[split_idx['train']])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            \n",
    "            # Validation loss\n",
    "            val_loss = criterion(out[split_idx['val']], data.y[split_idx['val']])\n",
    "            \n",
    "            # Accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            train_correct = pred[split_idx['train']].eq(data.y[split_idx['train']]).sum().item()\n",
    "            train_acc = train_correct / len(split_idx['train'])\n",
    "            \n",
    "            val_correct = pred[split_idx['val']].eq(data.y[split_idx['val']]).sum().item()\n",
    "            val_acc = val_correct / len(split_idx['val'])\n",
    "            \n",
    "            # Calculate AUC if binary classification\n",
    "            num_classes = torch.unique(data.y).shape[0]\n",
    "            if num_classes == 2:\n",
    "                try:\n",
    "                    train_probs = torch.exp(out[split_idx['train'], 1]).cpu().numpy()\n",
    "                    train_labels = data.y[split_idx['train']].cpu().numpy()\n",
    "                    val_probs = torch.exp(out[split_idx['val'], 1]).cpu().numpy()\n",
    "                    val_labels = data.y[split_idx['val']].cpu().numpy()\n",
    "                    \n",
    "                    train_auc = roc_auc_score(train_labels, train_probs)\n",
    "                    val_auc = roc_auc_score(val_labels, val_probs)\n",
    "                    \n",
    "                    history['train_auc'].append(train_auc)\n",
    "                    history['val_auc'].append(val_auc)\n",
    "                except Exception as e:\n",
    "                    # If AUC calculation fails, skip it\n",
    "                    logger.warning(f\"AUC calculation failed: {str(e)}\")\n",
    "                    history['train_auc'].append(0)\n",
    "                    history['val_auc'].append(0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # Track history\n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                       f\"Train Loss: {train_loss:.4f} | \"\n",
    "                       f\"Val Loss: {val_loss:.4f} | \"\n",
    "                       f\"Train Acc: {train_acc:.4f} | \"\n",
    "                       f\"Val Acc: {val_acc:.4f} | \"\n",
    "                       f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Check early stopping\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Training completed in {total_time:.2f}s | Best epoch: {best_epoch+1}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, f'{model_name}_best.pt')\n",
    "    torch.save(best_model_state, model_path)\n",
    "    logger.info(f\"Saved best model to {model_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(model_dir, f'{model_name}_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    logger.info(f\"Saved training history to {history_path}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training and validation loss and accuracy.\"\"\"\n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_title(f'{model_name} - Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    ax2.set_title(f'{model_name} - Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If AUC is available (binary classification), plot it as well\n",
    "    if 'train_auc' in history and history['train_auc']:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs, history['train_auc'], 'b-', label='Training AUC')\n",
    "        plt.plot(epochs, history['val_auc'], 'r-', label='Validation AUC')\n",
    "        plt.title(f'{model_name} - ROC AUC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now, let's train each of the GNN models on our transaction dataset. We'll define specific training functions for each model type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GCN Model\n",
    "\n",
    "First, let's train the Graph Convolutional Network (GCN) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gcn(data, split_idx, hidden_dim=256, num_layers=3, \n",
    "             dropout=0.5, lr=0.01, weight_decay=5e-4, \n",
    "             epochs=200, patience=20, device='cpu', model_dir='models'):\n",
    "    \"\"\"\n",
    "    Train a GCN model with the given parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : torch_geometric.data.Data\n",
    "        The graph data\n",
    "    split_idx : dict\n",
    "        Dictionary containing indices for train/val/test splits\n",
    "    hidden_dim : int\n",
    "        Dimension of hidden layers\n",
    "    num_layers : int\n",
    "        Number of GCN layers\n",
    "    dropout : float\n",
    "        Dropout probability\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    weight_decay : float\n",
    "        Weight decay factor\n",
    "    epochs : int\n",
    "        Maximum number of epochs\n",
    "    patience : int\n",
    "        Patience for early stopping\n",
    "    device : str\n",
    "        Device to use ('cpu' or 'cuda')\n",
    "    model_dir : str\n",
    "        Directory to save the model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : torch.nn.Module\n",
    "        The trained model\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    input_dim = data.x.shape[1]\n",
    "    output_dim = len(torch.unique(data.y))\n",
    "    \n",
    "    model = GCNModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=1e-5, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        split_idx=split_idx,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        model_dir=model_dir,\n",
    "        model_name='gcn'\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Define hyperparameters\n",
    "gcn_params = {\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epochs': 200,\n",
    "    'patience': 20,\n",
    "    'device': device,\n",
    "    'model_dir': 'models'\n",
    "}\n",
    "\n",
    "# Train GCN model\n",
    "logger.info(\"Training GCN model\")\n",
    "gcn_model, gcn_history = train_gcn(data, split_idx, **gcn_params)\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(gcn_history, 'GCN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GraphSAGE Model\n",
    "\n",
    "Next, let's train the GraphSAGE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sage(data, split_idx, hidden_dim=256, num_layers=3, \n",
    "              dropout=0.5, lr=0.01, weight_decay=5e-4, \n",
    "              epochs=200, patience=20, device='cpu', model_dir='models'):\n",
    "    \"\"\"\n",
    "    Train a GraphSAGE model with the given parameters.\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    input_dim = data.x.shape[1]\n",
    "    output_dim = len(torch.unique(data.y))\n",
    "    \n",
    "    model = SAGEModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=1e-5, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        split_idx=split_idx,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        model_dir=model_dir,\n",
    "        model_name='sage'\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Define hyperparameters\n",
    "sage_params = {\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epochs': 200,\n",
    "    'patience': 20,\n",
    "    'device': device,\n",
    "    'model_dir': 'models'\n",
    "}\n",
    "\n",
    "# Train GraphSAGE model\n",
    "logger.info(\"Training GraphSAGE model\")\n",
    "sage_model, sage_history = train_sage(data, split_idx, **sage_params)\n",
    "\n",
    "# Visualize training history\n",
    "plot_training_history(sage_history, 'GraphSAGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GAT Model\n",
    "\n",
    "Finally, let's train the Graph Attention Network (GAT) model. GATs can be computationally more intensive due to the attention mechanism, so we'll make sure to handle potential memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat(data, split_idx, hidden_dim=256, num_layers=3, \n",
    "             heads=8, dropout=0.5, lr=0.01, weight_decay=5e-4, \n",
    "             epochs=200, patience=20, device='cpu', model_dir='models'):\n",
    "    \"\"\"\n",
    "    Train a GAT model with the given parameters.\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    input_dim = data.x.shape[1]\n",
    "    output_dim = len(torch.unique(data.y))\n",
    "    \n",
    "    model = GATModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_layers=num_layers,\n",
    "        heads=heads,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=1e-5, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        data=data,\n",
    "        split_idx=split_idx,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        device=device,\n",
    "        model_dir=model_dir,\n",
    "        model_name='gat'\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Define hyperparameters\n",
    "gat_params = {\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 3,\n",
    "    'heads': 8,\n",
    "    'dropout': 0.5,\n",
    "    'lr': 0.01,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epochs': 200,\n",
    "    'patience': 20,\n",
    "    'device': device,\n",
    "    'model_dir': 'models'\n",
    "}\n",
    "\n",
    "# Train GAT model\n",
    "try:\n",
    "    logger.info(\"Training GAT model\")\n",
    "    gat_model, gat_history = train_gat(data, split_idx, **gat_params)\n",
    "    \n",
    "    # Visualize training history\n",
    "    plot_training_history(gat_history, 'GAT')\n",
    "    \n",
    "    has_gat = True\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e).lower():\n",
    "        logger.warning(\"Not enough memory for GAT model. Trying with fewer attention heads...\")\n",
    "        try:\n",
    "            # Try with fewer attention heads\n",
    "            gat_params['heads'] = 4\n",
    "            gat_model, gat_history = train_gat(data, split_idx, **gat_params)\n",
    "            plot_training_history(gat_history, 'GAT (4 heads)')\n",
    "            has_gat = True\n",
    "        except RuntimeError:\n",
    "            logger.warning(\"Still not enough memory. Skipping GAT model.\")\n",
    "            has_gat = False\n",
    "    else:\n",
    "        logger.error(f\"Error training GAT model: {e}\")\n",
    "        has_gat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Now that we've trained all the models, let's compare their performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "logger.info(\"Model comparison:\")\n",
    "best_val_gcn = min(gcn_history['val_loss']) if gcn_history['val_loss'] else float('inf')\n",
    "best_val_sage = min(sage_history['val_loss']) if sage_history['val_loss'] else float('inf')\n",
    "\n",
    "best_val_acc_gcn = max(gcn_history['val_acc']) if gcn_history['val_acc'] else 0\n",
    "best_val_acc_sage = max(sage_history['val_acc']) if sage_history['val_acc'] else 0\n",
    "\n",
    "if 'val_auc' in gcn_history and gcn_history['val_auc']:\n",
    "    best_val_auc_gcn = max(gcn_history['val_auc'])\n",
    "    best_val_auc_sage = max(sage_history['val_auc'])\n",
    "    has_auc = True\n",
    "else:\n",
    "    has_auc = False\n",
    "\n",
    "comparison = {\n",
    "    'gcn': {'val_loss': best_val_gcn, 'val_acc': best_val_acc_gcn},\n",
    "    'sage': {'val_loss': best_val_sage, 'val_acc': best_val_acc_sage}\n",
    "}\n",
    "\n",
    "if has_auc:\n",
    "    comparison['gcn']['val_auc'] = best_val_auc_gcn\n",
    "    comparison['sage']['val_auc'] = best_val_auc_sage\n",
    "\n",
    "if 'has_gat' in locals() and has_gat:\n",
    "    best_val_gat = min(gat_history['val_loss']) if gat_history['val_loss'] else float('inf')\n",
    "    best_val_acc_gat = max(gat_history['val_acc']) if gat_history['val_acc'] else 0\n",
    "    \n",
    "    comparison['gat'] = {'val_loss': best_val_gat, 'val_acc': best_val_acc_gat}\n",
    "    \n",
    "    if has_auc:\n",
    "        best_val_auc_gat = max(gat_history['val_auc'])\n",
    "        comparison['gat']['val_auc'] = best_val_auc_gat\n",
    "\n",
    "# Create a DataFrame for comparison\n",
    "comparison_df = pd.DataFrame()\n",
    "for model_name, metrics in comparison.items():\n",
    "    model_df = pd.DataFrame({**{'model': model_name}, **metrics}, index=[0])\n",
    "    comparison_df = pd.concat([comparison_df, model_df], ignore_index=True)\n",
    "\n",
    "# Sort by validation loss\n",
    "comparison_df = comparison_df.sort_values('val_loss')\n",
    "\n",
    "# Display comparison results\n",
    "print(\"Model comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Determine the best model\n",
    "best_model = comparison_df.iloc[0]['model']\n",
    "print(f\"\\nBest model based on validation loss: {best_model}\")\n",
    "best_metrics = comparison[best_model]\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save model comparison results\n",
    "with open(os.path.join('models', 'model_comparison.json'), 'w') as f:\n",
    "    json.dump(comparison, f, indent=4)\n",
    "print(\"\\nSaved model comparison results to models/model_comparison.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a visualization to compare the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot for validation loss\n",
    "sns.barplot(x='model', y='val_loss', data=comparison_df, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Validation Loss by Model')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Bar plot for validation accuracy\n",
    "sns.barplot(x='model', y='val_acc', data=comparison_df, ax=axes[1], palette='viridis')\n",
    "axes[1].set_title('Validation Accuracy by Model')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Validation Accuracy')\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation AUC if available\n",
    "if has_auc:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x='model', y='val_auc', data=comparison_df, palette='viridis')\n",
    "    plt.title('Validation AUC by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Validation AUC')\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Models\n",
    "\n",
    "Finally, let's save the best model for use in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the best model to 'best_model.pt'\n",
    "best_model_path = os.path.join('models', f'{best_model}_best.pt')\n",
    "import shutil\n",
    "shutil.copy(best_model_path, os.path.join('models', 'best_model.pt'))\n",
    "\n",
    "# Save the name of the best model\n",
    "with open(os.path.join('models', 'best_model_name.txt'), 'w') as f:\n",
    "    f.write(best_model)\n",
    "\n",
    "print(f\"Saved best model ({best_model}) as 'best_model.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've implemented and trained three different Graph Neural Network architectures for Bitcoin transaction fraud detection:\n",
    "\n",
    "1. Graph Convolutional Network (GCN)\n",
    "2. GraphSAGE\n",
    "3. Graph Attention Network (GAT)\n",
    "\n",
    "We've compared their performance on the validation set and identified the best model based on validation loss. The best model has been saved for use in the evaluation notebook.\n",
    "\n",
    "Key findings:\n",
    "- Each model architecture has its strengths and weaknesses\n",
    "- The best model for this dataset is `{best_model}`\n",
    "- The models were trained with early stopping to prevent overfitting\n",
    "- We've visualized the training history to better understand the learning process\n",
    "\n",
    "In the next notebook, we'll evaluate the best model on the test set and analyze its performance in detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
