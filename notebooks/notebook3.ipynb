{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain Fraud Detection: Model Development\n",
    "\n",
    "This notebook focuses on developing Graph Neural Network (GNN) models for blockchain fraud detection. We'll use the processed features and graph structure from the previous notebooks to train and compare different GNN architectures.\n",
    "\n",
    "We'll implement:\n",
    "1. Graph Convolutional Networks (GCN)\n",
    "2. GraphSAGE\n",
    "3. Graph Attention Networks (GAT)\n",
    "\n",
    "And explore various hyperparameters and training strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create directories for saving models and results\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../reports', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from processed directory\n",
    "try:\n",
    "    # Try loading the complete Data object\n",
    "    data = torch.load('../data/processed/data.pt')\n",
    "    print(f\"Loaded PyTorch Geometric Data object: {data}\")\n",
    "except FileNotFoundError:\n",
    "    # If not found, load individual components\n",
    "    features = np.load('../data/processed/features.npy')\n",
    "    labels = np.load('../data/processed/labels.npy')\n",
    "    edge_index = torch.load('../data/processed/edge_index.pt')\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    x = torch.FloatTensor(features)\n",
    "    y = torch.LongTensor(labels)\n",
    "    \n",
    "    # Create Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    print(f\"Created PyTorch Geometric Data object from components\")\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data dimensions\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {len(torch.unique(data.y))}\")\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = torch.bincount(data.y)\n",
    "print(f\"Class distribution: {class_counts.cpu().numpy()}\")\n",
    "print(f\"Fraud ratio: {class_counts[1].item() / class_counts.sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits\n",
    "def create_data_splits(data, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    \"\"\"Create train/val/test splits for node classification.\"\"\"\n",
    "    # Get indices for all nodes\n",
    "    indices = np.arange(data.num_nodes)\n",
    "    \n",
    "    # First split: train vs. (val+test)\n",
    "    train_idx, temp_idx = train_test_split(\n",
    "        indices, \n",
    "        train_size=train_size, \n",
    "        random_state=random_state,\n",
    "        stratify=data.y.cpu().numpy()  # Stratify by class\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs. test\n",
    "    val_test_ratio = val_size / (val_size + test_size)\n",
    "    val_idx, test_idx = train_test_split(\n",
    "        temp_idx,\n",
    "        train_size=val_test_ratio,\n",
    "        random_state=random_state,\n",
    "        stratify=data.y[temp_idx].cpu().numpy()  # Stratify by class\n",
    "    )\n",
    "    \n",
    "    # Return split indices\n",
    "    return {\n",
    "        'train': train_idx,\n",
    "        'val': val_idx,\n",
    "        'test': test_idx\n",
    "    }\n",
    "\n",
    "# Create splits\n",
    "split_idx = create_data_splits(data)\n",
    "\n",
    "# Print split sizes\n",
    "print(f\"Train set size: {len(split_idx['train'])}\")\n",
    "print(f\"Validation set size: {len(split_idx['val'])}\")\n",
    "print(f\"Test set size: {len(split_idx['test'])}\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_y = data.y[split_idx[split]].cpu().numpy()\n",
    "    unique, counts = np.unique(split_y, return_counts=True)\n",
    "    print(f\"{split.capitalize()} set class distribution: {dict(zip(unique, counts))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define GNN Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCN model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                 dropout=0.5, batch_norm=True, residual=True):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs = nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev\n",
    "            \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GraphSAGE model\n",
    "class SAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                 dropout=0.5, batch_norm=True, residual=True, aggr='mean'):\n",
    "        super(SAGEModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        # Input layer\n",
    "        self.convs = nn.ModuleList([SAGEConv(input_dim, hidden_dim, aggr=aggr)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim, aggr=aggr))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(SAGEConv(hidden_dim, output_dim, aggr=aggr))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev\n",
    "            \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAT model\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                 dropout=0.5, batch_norm=True, residual=True, heads=8):\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        self.residual = residual\n",
    "        \n",
    "        # Input layer with multiple attention heads\n",
    "        self.convs = nn.ModuleList([GATConv(input_dim, hidden_dim // heads, heads=heads)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_dim, hidden_dim // heads, heads=heads))\n",
    "        \n",
    "        # Output layer (with 1 attention head)\n",
    "        self.convs.append(GATConv(hidden_dim, output_dim, heads=1))\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        if batch_norm:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        if self.batch_norm:\n",
    "            for bn in self.bns:\n",
    "                bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # Input layer\n",
    "        h = self.convs[0](x, edge_index)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Hidden layers with residual connections\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            h_prev = h\n",
    "            h = self.convs[i](h, edge_index)\n",
    "            \n",
    "            if self.batch_norm:\n",
    "                h = self.bns[i-1](h)\n",
    "            \n",
    "            h = F.relu(h)\n",
    "            \n",
    "            if self.residual:\n",
    "                h = h + h_prev\n",
    "            \n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Output layer\n",
    "        h = self.convs[-1](h, edge_index)\n",
    "        \n",
    "        return F.log_softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, split_idx, optimizer, criterion, \n",
    "               scheduler=None, epochs=200, patience=20, \n",
    "               verbose=True, model_name=\"gnn\"):\n",
    "    \"\"\"Train a GNN model with early stopping.\"\"\"\n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'train_auc': [],\n",
    "        'val_auc': []\n",
    "    }\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_auc = 0.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Train phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        train_loss = criterion(out[split_idx['train']], data.y[split_idx['train']])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            \n",
    "            # Loss\n",
    "            val_loss = criterion(out[split_idx['val']], data.y[split_idx['val']])\n",
    "            \n",
    "            # Accuracy\n",
    "            pred = out.argmax(dim=1)\n",
    "            train_acc = (pred[split_idx['train']] == data.y[split_idx['train']]).sum().item() / len(split_idx['train'])\n",
    "            val_acc = (pred[split_idx['val']] == data.y[split_idx['val']]).sum().item() / len(split_idx['val'])\n",
    "            \n",
    "            # AUC\n",
    "            train_probs = torch.exp(out[split_idx['train'], 1]).cpu().numpy()\n",
    "            val_probs = torch.exp(out[split_idx['val'], 1]).cpu().numpy()\n",
    "            train_auc = roc_auc_score(data.y[split_idx['train']].cpu().numpy(), train_probs)\n",
    "            val_auc = roc_auc_score(data.y[split_idx['val']].cpu().numpy(), val_probs)\n",
    "        \n",
    "        # Update learning rate with scheduler if provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_auc)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        # Print progress\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "                  f\"Train AUC: {train_auc:.4f} | Val AUC: {val_auc:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            if verbose:\n",
    "                print(f\"    > New best model with validation AUC: {val_auc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Training summary\n",
    "    total_time = time.time() - start_time\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best model at epoch {best_epoch+1} with validation AUC: {best_val_auc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save best model\n",
    "    torch.save(best_model_state, f\"../models/{model_name}_best.pt\")\n",
    "    \n",
    "    # Save training history\n",
    "    import json\n",
    "    with open(f\"../models/{model_name}_history.json\", 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCN model hyperparameters\n",
    "gcn_params = {\n",
    "    'input_dim': data.num_features,\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': 2,  # Binary classification\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'batch_norm': True,\n",
    "    'residual': True\n",
    "}\n",
    "\n",
    "# Initialize GCN model\n",
    "gcn_model = GCNModel(**gcn_params).to(device)\n",
    "gcn_model.reset_parameters()\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10, min_lr=1e-5, verbose=True\n",
    ")\n",
    "\n",
    "# Train GCN model\n",
    "print(\"Training GCN model...\")\n",
    "gcn_model, gcn_history = train_model(\n",
    "    model=gcn_model,\n",
    "    data=data,\n",
    "    split_idx=split_idx,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    epochs=200,\n",
    "    patience=20,\n",
    "    model_name=\"gcn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GCN training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(gcn_history['train_loss'], label='Train')\n",
    "axes[0].plot(gcn_history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Loss', fontsize=15)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(gcn_history['train_acc'], label='Train')\n",
    "axes[1].plot(gcn_history['val_acc'], label='Validation')\n",
    "axes[1].set_title('Accuracy', fontsize=15)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# AUC plot\n",
    "axes[2].plot(gcn_history['train_auc'], label='Train')\n",
    "axes[2].plot(gcn_history['val_auc'], label='Validation')\n",
    "axes[2].set_title('ROC AUC', fontsize=15)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('AUC', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/gcn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train GraphSAGE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GraphSAGE model hyperparameters\n",
    "sage_params = {\n",
    "    'input_dim': data.num_features,\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': 2,  # Binary classification\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.5,\n",
    "    'batch_norm': True,\n",
    "    'residual': True,\n",
    "    'aggr': 'mean'  # Aggregation method: 'mean', 'max', or 'sum'\n",
    "}\n",
    "\n",
    "# Initialize GraphSAGE model\n",
    "sage_model = SAGEModel(**sage_params).to(device)\n",
    "sage_model.reset_parameters()\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10, min_lr=1e-5, verbose=True\n",
    ")\n",
    "\n",
    "# Train GraphSAGE model\n",
    "print(\"Training GraphSAGE model...\")\n",
    "sage_model, sage_history = train_model(\n",
    "    model=sage_model,\n",
    "    data=data,\n",
    "    split_idx=split_idx,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    epochs=200,\n",
    "    patience=20,\n",
    "    model_name=\"sage\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GraphSAGE training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(sage_history['train_loss'], label='Train')\n",
    "axes[0].plot(sage_history['val_loss'], label='Validation')\n",
    "axes[0].set_title('Loss', fontsize=15)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(sage_history['train_acc'], label='Train')\n",
    "axes[1].plot(sage_history['val_acc'], label='Validation')\n",
    "axes[1].set_title('Accuracy', fontsize=15)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# AUC plot\n",
    "axes[2].plot(sage_history['train_auc'], label='Train')\n",
    "axes[2].plot(sage_history['val_auc'], label='Validation')\n",
    "axes[2].set_title('ROC AUC', fontsize=15)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('AUC', fontsize=12)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/sage_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train GAT Model (Optional, requires more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have enough memory for GAT (more memory-intensive)\n",
    "try:\n",
    "    # Define GAT model hyperparameters\n",
    "    gat_params = {\n",
    "        'input_dim': data.num_features,\n",
    "        'hidden_dim': 256,\n",
    "        'output_dim': 2,  # Binary classification\n",
    "        'num_layers': 3,\n",
    "        'dropout': 0.5,\n",
    "        'batch_norm': True,\n",
    "        'residual': True,\n",
    "        'heads': 8  # Number of attention heads\n",
    "    }\n",
    "    \n",
    "    # Initialize GAT model\n",
    "    gat_model = GATModel(**gat_params).to(device)\n",
    "    gat_model.reset_parameters()\n",
    "    \n",
    "    # Set optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=10, min_lr=1e-5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Train GAT model\n",
    "    print(\"Training GAT model...\")\n",
    "    gat_model, gat_history = train_model(\n",
    "        model=gat_model,\n",
    "        data=data,\n",
    "        split_idx=split_idx,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "        model_name=\"gat\"\n",
    "    )\n",
    "    \n",
    "    # Plot GAT training history\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(gat_history['train_loss'], label='Train')\n",
    "    axes[0].plot(gat_history['val_loss'], label='Validation')\n",
    "    axes[0].set_title('Loss', fontsize=15)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(gat_history['train_acc'], label='Train')\n",
    "    axes[1].plot(gat_history['val_acc'], label='Validation')\n",
    "    axes[1].set_title('Accuracy', fontsize=15)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # AUC plot\n",
    "    axes[2].plot(gat_history['train_auc'], label='Train')\n",
    "    axes[2].plot(gat_history['val_auc'], label='Validation')\n",
    "    axes[2].set_title('ROC AUC', fontsize=15)\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('AUC', fontsize=12)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/gat_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e).lower():\n",
    "        print(\"Not enough GPU memory for GAT model. Skipping GAT training.\")\n",
    "        print(\"You can try reducing the hidden dimension or number of attention heads.\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to explore hyperparameter tuning\n",
    "\n",
    "# def hyperparameter_tuning(model_class, param_grid, data, split_idx, epochs=100, patience=10):\n",
    "#     \"\"\"Perform hyperparameter tuning on a model class.\"\"\"\n",
    "#     results = []\n",
    "#     \n",
    "#     # Generate parameter combinations\n",
    "#     import itertools\n",
    "#     param_names = param_grid.keys()\n",
    "#     param_values = list(param_grid.values())\n",
    "#     param_combinations = list(itertools.product(*param_values))\n",
    "#     \n",
    "#     print(f\"Testing {len(param_combinations)} parameter combinations\")\n",
    "#     \n",
    "#     for i, params in enumerate(param_combinations):\n",
    "#         param_dict = {name: value for name, value in zip(param_names, params)}\n",
    "#         print(f\"\\nCombination {i+1}/{len(param_combinations)}: {param_dict}\")\n",
    "#         \n",
    "#         # Initialize model with parameters\n",
    "#         model = model_class(**param_dict).to(device)\n",
    "#         model.reset_parameters()\n",
    "#         \n",
    "#         # Set optimizer and loss function\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "#         criterion = torch.nn.NLLLoss()\n",
    "#         \n",
    "#         # Train model\n",
    "#         model, history = train_model(\n",
    "#             model=model,\n",
    "#             data=data,\n",
    "#             split_idx=split_idx,\n",
    "#             optimizer=optimizer,\n",
    "#             criterion=criterion,\n",
    "#             epochs=epochs,\n",
    "#             patience=patience,\n",
    "#             verbose=False\n",
    "#         )\n",
    "#         \n",
    "#         # Get best validation performance\n",
    "#         best_val_auc = max(history['val_auc'])\n",
    "#         best_epoch = history['val_auc'].index(best_val_auc)\n",
    "#         \n",
    "#         # Record results\n",
    "#         results.append({\n",
    "#             'params': param_dict,\n",
    "#             'val_auc': best_val_auc,\n",
    "#             'best_epoch': best_epoch\n",
    "#         })\n",
    "#         \n",
    "#         print(f\"Best validation AUC: {best_val_auc:.4f} at epoch {best_epoch+1}\")\n",
    "#     \n",
    "#     # Sort results by validation AUC\n",
    "#     results.sort(key=lambda x: x['val_auc'], reverse=True)\n",
    "#     \n",
    "#     # Return best parameters\n",
    "#     return results\n",
    "\n",
    "# # GCN hyperparameter grid\n",
    "# gcn_param_grid = {\n",
    "#     'input_dim': [data.num_features],\n",
    "#     'hidden_dim': [128, 256],\n",
    "#     'output_dim': [2],\n",
    "#     'num_layers': [2, 3],\n",
    "#     'dropout': [0.3, 0.5],\n",
    "#     'batch_norm': [True, False],\n",
    "#     'residual': [True, False]\n",
    "# }\n",
    "\n",
    "# # Run hyperparameter tuning\n",
    "# gcn_tuning_results = hyperparameter_tuning(GCNModel, gcn_param_grid, data, split_idx)\n",
    "\n",
    "# # Print best results\n",
    "# print(\"\\nTop 3 parameter combinations:\")\n",
    "# for i, result in enumerate(gcn_tuning_results[:3]):\n",
    "#     print(f\"{i+1}. Val AUC: {result['val_auc']:.4f}, Params: {result['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training histories\n",
    "import json\n",
    "\n",
    "with open('../models/gcn_history.json', 'r') as f:\n",
    "    gcn_history = json.load(f)\n",
    "\n",
    "with open('../models/sage_history.json', 'r') as f:\n",
    "    sage_history = json.load(f)\n",
    "\n",
    "# Check if GAT history exists\n",
    "try:\n",
    "    with open('../models/gat_history.json', 'r') as f:\n",
    "        gat_history = json.load(f)\n",
    "    has_gat = True\n",
    "except FileNotFoundError:\n",
    "    has_gat = False\n",
    "\n",
    "# Collect best validation metrics\n",
    "best_metrics = {\n",
    "    'GCN': {\n",
    "        'val_auc': max(gcn_history['val_auc']),\n",
    "        'epoch': gcn_history['val_auc'].index(max(gcn_history['val_auc'])) + 1\n",
    "    },\n",
    "    'GraphSAGE': {\n",
    "        'val_auc': max(sage_history['val_auc']),\n",
    "        'epoch': sage_history['val_auc'].index(max(sage_history['val_auc'])) + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "if has_gat:\n",
    "    best_metrics['GAT'] = {\n",
    "        'val_auc': max(gat_history['val_auc']),\n",
    "        'epoch': gat_history['val_auc'].index(max(gat_history['val_auc'])) + 1\n",
    "    }\n",
    "\n",
    "# Print comparison\n",
    "print(\"Model Performance Comparison (Validation Set):\")\n",
    "for model_name, metrics in best_metrics.items():\n",
    "    print(f\"{model_name}: AUC = {metrics['val_auc']:.4f} (at epoch {metrics['epoch']})\")\n",
    "\n",
    "# Identify best model\n",
    "best_model = max(best_metrics.items(), key=lambda x: x[1]['val_auc'])[0]\n",
    "print(f\"\\nBest model based on validation AUC: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(gcn_history['val_auc'], label='GCN')\n",
    "plt.plot(sage_history['val_auc'], label='GraphSAGE')\n",
    "if has_gat:\n",
    "    plt.plot(gat_history['val_auc'], label='GAT')\n",
    "\n",
    "plt.title('Validation AUC Comparison', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('AUC', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for model_name, metrics in best_metrics.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Best Val AUC': metrics['val_auc'],\n",
    "        'Best Epoch': metrics['epoch'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('Best Val AUC', ascending=False)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to CSV\n",
    "summary_df.to_csv('../reports/model_comparison_summary.csv', index=False)\n",
    "print(\"Saved model comparison summary to ../reports/model_comparison_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the Best Model for Evaluation\n",
    "\n",
    "In the next notebook, we'll load the best model for a detailed evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model file path\n",
    "best_model_path = f\"../models/{best_model.lower()}_best.pt\"\n",
    "\n",
    "# Copy the best model to a common name for easier loading in the evaluation notebook\n",
    "import shutil\n",
    "shutil.copy(best_model_path, \"../models/best_model.pt\")\n",
    "print(f\"Copied {best_model_path} to ../models/best_model.pt for evaluation\")\n",
    "\n",
    "# Also save the model name\n",
    "with open(\"../models/best_model_name.txt\", \"w\") as f:\n",
    "    f.write(best_model)\n",
    "print(f\"Saved best model name ({best_model}) to ../models/best_model_name.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we developed and trained several Graph Neural Network models for blockchain fraud detection:\n",
    "\n",
    "1. We implemented GCN, GraphSAGE, and GAT models with advanced features like residual connections, batch normalization, and dropout.\n",
    "\n",
    "2. We trained these models using early stopping based on validation AUC to prevent overfitting.\n",
    "\n",
    "3. We compared model performances and identified the best model based on validation AUC.\n",
    "\n",
    "The best performing model was [Best Model], achieving a validation AUC of [Best AUC]. In the next notebook, we'll perform a detailed evaluation of this model on the test set to assess its generalization capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
