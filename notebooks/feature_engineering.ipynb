{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitcoin Transaction Fraud Detection: Feature Engineering\n",
    "\n",
    "This notebook focuses on extracting and analyzing graph-based features for the Bitcoin transaction dataset. Graph features can provide valuable information about the structure and patterns of transactions, which can help improve fraud detection.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#Setup)\n",
    "2. [Loading Processed Data](#Loading-Processed-Data)\n",
    "3. [Basic Graph Features](#Basic-Graph-Features)\n",
    "4. [Centrality Measures](#Centrality-Measures)\n",
    "5. [Clustering Coefficients](#Clustering-Coefficients)\n",
    "6. [Temporal Features](#Temporal-Features)\n",
    "7. [Combining Features](#Combining-Features)\n",
    "8. [Feature Importance Analysis](#Feature-Importance-Analysis)\n",
    "9. [Saving Engineered Features](#Saving-Engineered-Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import logging\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data/processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Data\n",
    "\n",
    "First, let's load the preprocessed data from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(classes_path, edgelist_path, features_path):\n",
    "    \"\"\"\n",
    "    Load blockchain dataset from CSV files with the special format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    classes_path : str\n",
    "        Path to the classes CSV file with txId and class columns\n",
    "    edgelist_path : str\n",
    "        Path to the edgelist CSV file with txId1 and txId2 columns\n",
    "    features_path : str\n",
    "        Path to the features CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data with txId, class, and features\n",
    "    df_edges : pandas.DataFrame\n",
    "        DataFrame containing edge data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from {classes_path}, {edgelist_path}, and {features_path}\")\n",
    "    \n",
    "    # Load classes data\n",
    "    df_classes = pd.read_csv(classes_path)\n",
    "    logger.info(f\"Loaded {len(df_classes)} transactions with class information\")\n",
    "    \n",
    "    # Load edge data\n",
    "    df_edges = pd.read_csv(edgelist_path)\n",
    "    logger.info(f\"Loaded {len(df_edges)} edges\")\n",
    "    \n",
    "    # Load features data - this has a non-standard format\n",
    "    try:\n",
    "        # Read the features file\n",
    "        df_features = pd.read_csv(features_path)\n",
    "        \n",
    "        # First column should be txId\n",
    "        df_features = df_features.rename(columns={df_features.columns[0]: 'txId'})\n",
    "        \n",
    "        # Second column is a constant (1) - drop it\n",
    "        df_features = df_features.drop(columns=[df_features.columns[1]])\n",
    "        \n",
    "        # Rename remaining columns to feature_0, feature_1, etc.\n",
    "        feature_cols = [col for col in df_features.columns if col != 'txId']\n",
    "        feature_rename = {col: f'feature_{i}' for i, col in enumerate(feature_cols)}\n",
    "        df_features = df_features.rename(columns=feature_rename)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_features.columns)-1} features for {len(df_features)} transactions\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading features: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Merge classes and features based on txId\n",
    "    df_nodes = pd.merge(df_classes, df_features, on='txId', how='inner')\n",
    "    logger.info(f\"Combined data has {len(df_nodes)} transactions with both class and feature information\")\n",
    "    \n",
    "    return df_nodes, df_edges\n",
    "\n",
    "def create_node_mapping(df_nodes):\n",
    "    \"\"\"\n",
    "    Create a mapping between transaction IDs and indices for graph construction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    id2idx : dict\n",
    "        Dictionary mapping transaction IDs to indices\n",
    "    \"\"\"\n",
    "    # Create mapping of txId to index\n",
    "    id2idx = {tx_id: i for i, tx_id in enumerate(df_nodes['txId'])}\n",
    "    logger.info(f\"Created mapping for {len(id2idx)} transactions\")\n",
    "    return id2idx\n",
    "\n",
    "# Try to load processed data from previous notebook\n",
    "processed_features_path = 'data/processed/features.npy'\n",
    "processed_labels_path = 'data/processed/labels.npy'\n",
    "\n",
    "if os.path.exists(processed_features_path) and os.path.exists(processed_labels_path):\n",
    "    logger.info(\"Loading processed data from previous notebook\")\n",
    "    features = np.load(processed_features_path)\n",
    "    labels = np.load(processed_labels_path)\n",
    "    logger.info(f\"Loaded {features.shape[0]} samples with {features.shape[1]} features\")\n",
    "    \n",
    "    # We also need to load the raw data for graph analysis\n",
    "    classes_path = 'data/raw/classes.csv'\n",
    "    edgelist_path = 'data/raw/edgelist.csv'\n",
    "    features_path = 'data/raw/Features.csv'\n",
    "    \n",
    "    if all(os.path.exists(p) for p in [classes_path, edgelist_path, features_path]):\n",
    "        df_nodes, df_edges = load_data(classes_path, edgelist_path, features_path)\n",
    "        \n",
    "        # Filter out unknown classes\n",
    "        unknown_mask = df_nodes['class'] == 'unknown'\n",
    "        if unknown_mask.sum() > 0:\n",
    "            df_nodes = df_nodes[~unknown_mask]\n",
    "        \n",
    "        # Convert class labels to numeric if needed\n",
    "        if df_nodes['class'].dtype == 'object':\n",
    "            unique_classes = df_nodes['class'].unique()\n",
    "            class_map = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "            df_nodes['class'] = df_nodes['class'].map(class_map)\n",
    "        \n",
    "        # Create node mapping\n",
    "        id2idx = create_node_mapping(df_nodes)\n",
    "        \n",
    "        logger.info(\"Raw data loaded for graph analysis\")\n",
    "    else:\n",
    "        logger.warning(\"Raw data files not found. Please run Data Preparation notebook first.\")\n",
    "else:\n",
    "    logger.warning(\"Processed data not found. Please run Data Preparation notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Graph Features\n",
    "\n",
    "Let's create a graph from the transaction data and compute basic graph metrics for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(df_nodes, df_edges):\n",
    "    \"\"\"\n",
    "    Create a directed graph from transaction data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        DataFrame containing node data\n",
    "    df_edges : pandas.DataFrame\n",
    "        DataFrame containing edge data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    G : networkx.DiGraph\n",
    "        Directed graph of transactions\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating transaction graph\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for _, row in df_nodes.iterrows():\n",
    "        node_id = row['txId']\n",
    "        is_fraud = row['class'] == 1\n",
    "        G.add_node(node_id, fraud=is_fraud)\n",
    "    \n",
    "    # Add edges\n",
    "    for _, row in df_edges.iterrows():\n",
    "        source_id, target_id = row['txId1'], row['txId2']\n",
    "        if source_id in G and target_id in G:\n",
    "            G.add_edge(source_id, target_id)\n",
    "    \n",
    "    logger.info(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Create graph\n",
    "G = create_graph(df_nodes, df_edges)\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"Graph statistics:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Is directed: {nx.is_directed(G)}\")\n",
    "print(f\"Is connected: {nx.is_weakly_connected(G)}\")\n",
    "\n",
    "# Get number of connected components\n",
    "weakly_connected_components = list(nx.weakly_connected_components(G))\n",
    "print(f\"Number of weakly connected components: {len(weakly_connected_components)}\")\n",
    "\n",
    "# Get size of largest component\n",
    "largest_component_size = max(len(c) for c in weakly_connected_components)\n",
    "print(f\"Size of largest weakly connected component: {largest_component_size} nodes ({largest_component_size/G.number_of_nodes():.2%} of all nodes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the degree distribution of the network to better understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in-degree and out-degree distributions\n",
    "in_degrees = [d for n, d in G.in_degree()]\n",
    "out_degrees = [d for n, d in G.out_degree()]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot in-degree distribution\n",
    "axes[0].hist(in_degrees, bins=50, alpha=0.7, color='blue')\n",
    "axes[0].set_title('In-Degree Distribution')\n",
    "axes[0].set_xlabel('In-Degree')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot out-degree distribution\n",
    "axes[1].hist(out_degrees, bins=50, alpha=0.7, color='green')\n",
    "axes[1].set_title('Out-Degree Distribution')\n",
    "axes[1].set_xlabel('Out-Degree')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print degree statistics\n",
    "print(\"In-degree statistics:\")\n",
    "print(f\"Min: {min(in_degrees)}\")\n",
    "print(f\"Max: {max(in_degrees)}\")\n",
    "print(f\"Mean: {np.mean(in_degrees):.2f}\")\n",
    "print(f\"Median: {np.median(in_degrees)}\")\n",
    "\n",
    "print(\"\\nOut-degree statistics:\")\n",
    "print(f\"Min: {min(out_degrees)}\")\n",
    "print(f\"Max: {max(out_degrees)}\")\n",
    "print(f\"Mean: {np.mean(out_degrees):.2f}\")\n",
    "print(f\"Median: {np.median(out_degrees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the degree distributions between fraudulent and legitimate transactions to see if there are any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate nodes by class\n",
    "fraudulent_nodes = [node for node in G.nodes() if G.nodes[node].get('fraud', False)]\n",
    "legitimate_nodes = [node for node in G.nodes() if not G.nodes[node].get('fraud', False)]\n",
    "\n",
    "print(f\"Number of fraudulent nodes: {len(fraudulent_nodes)}\")\n",
    "print(f\"Number of legitimate nodes: {len(legitimate_nodes)}\")\n",
    "\n",
    "# Get in-degree and out-degree for each class\n",
    "fraud_in_degrees = [G.in_degree(node) for node in fraudulent_nodes]\n",
    "fraud_out_degrees = [G.out_degree(node) for node in fraudulent_nodes]\n",
    "\n",
    "legit_in_degrees = [G.in_degree(node) for node in legitimate_nodes]\n",
    "legit_out_degrees = [G.out_degree(node) for node in legitimate_nodes]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot in-degree distribution by class\n",
    "axes[0].hist(legit_in_degrees, bins=30, alpha=0.5, color='blue', label='Legitimate')\n",
    "axes[0].hist(fraud_in_degrees, bins=30, alpha=0.5, color='red', label='Fraudulent')\n",
    "axes[0].set_title('In-Degree Distribution by Class')\n",
    "axes[0].set_xlabel('In-Degree')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot out-degree distribution by class\n",
    "axes[1].hist(legit_out_degrees, bins=30, alpha=0.5, color='blue', label='Legitimate')\n",
    "axes[1].hist(fraud_out_degrees, bins=30, alpha=0.5, color='red', label='Fraudulent')\n",
    "axes[1].set_title('Out-Degree Distribution by Class')\n",
    "axes[1].set_xlabel('Out-Degree')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print degree statistics by class\n",
    "print(\"Fraudulent transactions:\")\n",
    "print(f\"Average in-degree: {np.mean(fraud_in_degrees):.2f}\")\n",
    "print(f\"Average out-degree: {np.mean(fraud_out_degrees):.2f}\")\n",
    "\n",
    "print(\"\\nLegitimate transactions:\")\n",
    "print(f\"Average in-degree: {np.mean(legit_in_degrees):.2f}\")\n",
    "print(f\"Average out-degree: {np.mean(legit_out_degrees):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality Measures\n",
    "\n",
    "Now, let's compute centrality measures for each node. These measures can help identify important nodes in the network and might be useful for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fast_graph_features(df_nodes, G):\n",
    "    \"\"\"\n",
    "    Compute optimized graph features that are fast to calculate.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        Dataframe containing node data\n",
    "    G : networkx.DiGraph\n",
    "        NetworkX graph of transactions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    graph_features : numpy.ndarray\n",
    "        Array of graph features for each node\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing optimized graph features (fast version)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate degrees (fast)\n",
    "    logger.info(\"Computing degree centralities...\")\n",
    "    in_degree = dict(G.in_degree())\n",
    "    out_degree = dict(G.out_degree())\n",
    "    total_degree = {node: in_degree.get(node, 0) + out_degree.get(node, 0) for node in G.nodes()}\n",
    "    \n",
    "    logger.info(f\"Calculated degree centralities in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    # PageRank (reasonably fast)\n",
    "    logger.info(\"Computing PageRank centrality...\")\n",
    "    pagerank_start = time.time()\n",
    "    try:\n",
    "        pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "        logger.info(f\"Calculated PageRank in {time.time()-pagerank_start:.2f} seconds\")\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        logger.warning(\"PageRank failed to converge, using simplified calculation\")\n",
    "        pagerank = nx.pagerank(G, alpha=0.85, max_iter=50, tol=1e-3)\n",
    "        logger.info(f\"Calculated simplified PageRank in {time.time()-pagerank_start:.2f} seconds\")\n",
    "    \n",
    "    # HITS algorithm (good for directed networks)\n",
    "    logger.info(\"Computing HITS centrality...\")\n",
    "    hits_start = time.time()\n",
    "    try:\n",
    "        hubs, authorities = nx.hits(G, max_iter=100)\n",
    "        logger.info(f\"Calculated HITS in {time.time()-hits_start:.2f} seconds\")\n",
    "    except nx.PowerIterationFailedConvergence:\n",
    "        logger.warning(\"HITS failed to converge, using simplified calculation\")\n",
    "        hubs, authorities = nx.hits(G, max_iter=50, tol=1e-3)\n",
    "        logger.info(f\"Calculated simplified HITS in {time.time()-hits_start:.2f} seconds\")\n",
    "    \n",
    "    # Create feature matrix with 5 features (in-degree, out-degree, total degree, pagerank, hubs, authorities)\n",
    "    logger.info(\"Creating feature matrix...\")\n",
    "    graph_features = np.zeros((len(df_nodes), 6))\n",
    "    \n",
    "    logger.info(\"Filling feature matrix...\")\n",
    "    for i, node_id in tqdm(enumerate(df_nodes['txId']), desc=\"Processing nodes\", total=len(df_nodes)):\n",
    "        # In-degree\n",
    "        graph_features[i, 0] = in_degree.get(node_id, 0)\n",
    "        # Out-degree\n",
    "        graph_features[i, 1] = out_degree.get(node_id, 0)\n",
    "        # Total degree\n",
    "        graph_features[i, 2] = total_degree.get(node_id, 0)\n",
    "        # PageRank\n",
    "        graph_features[i, 3] = pagerank.get(node_id, 0)\n",
    "        # Hub score\n",
    "        graph_features[i, 4] = hubs.get(node_id, 0)\n",
    "        # Authority score\n",
    "        graph_features[i, 5] = authorities.get(node_id, 0)\n",
    "    \n",
    "    logger.info(f\"Generated graph features with shape {graph_features.shape} in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    return graph_features\n",
    "\n",
    "# Compute centrality measures\n",
    "centrality_features = compute_fast_graph_features(df_nodes, G)\n",
    "\n",
    "# Create a DataFrame with the centrality features\n",
    "centrality_df = pd.DataFrame(\n",
    "    centrality_features,\n",
    "    columns=['in_degree', 'out_degree', 'total_degree', 'pagerank', 'hub_score', 'authority_score']\n",
    ")\n",
    "\n",
    "# Add transaction ID and class for reference\n",
    "centrality_df['txId'] = df_nodes['txId'].values\n",
    "centrality_df['class'] = df_nodes['class'].values\n",
    "\n",
    "# Display statistics of centrality features\n",
    "print(\"Centrality features statistics:\")\n",
    "centrality_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of each centrality measure for fraudulent and legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 3 rows and 2 columns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "centrality_measures = ['in_degree', 'out_degree', 'total_degree', 'pagerank', 'hub_score', 'authority_score']\n",
    "\n",
    "for i, measure in enumerate(centrality_measures):\n",
    "    # Filter values for each class\n",
    "    fraud_values = centrality_df[centrality_df['class'] == 1][measure]\n",
    "    legit_values = centrality_df[centrality_df['class'] == 0][measure]\n",
    "    \n",
    "    # Plot distributions\n",
    "    if measure in ['in_degree', 'out_degree', 'total_degree']:  # Discrete values\n",
    "        axes[i].hist(legit_values, bins=30, alpha=0.5, color='blue', label='Legitimate')\n",
    "        axes[i].hist(fraud_values, bins=30, alpha=0.5, color='red', label='Fraudulent')\n",
    "    else:  # Continuous values\n",
    "        sns.kdeplot(legit_values, ax=axes[i], color='blue', label='Legitimate', fill=True, alpha=0.3)\n",
    "        sns.kdeplot(fraud_values, ax=axes[i], color='red', label='Fraudulent', fill=True, alpha=0.3)\n",
    "    \n",
    "    axes[i].set_title(f'{measure.replace(\"_\", \" \").title()} Distribution by Class')\n",
    "    axes[i].set_xlabel(measure.replace(\"_\", \" \").title())\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Use log scale for degree measures\n",
    "    if measure in ['in_degree', 'out_degree', 'total_degree']:\n",
    "        axes[i].set_yscale('log')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the average values for each class\n",
    "print(\"Average centrality measures by class:\")\n",
    "print(centrality_df.groupby('class')[centrality_measures].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the correlation between centrality measures and the target class to see which measures are most predictive of fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with the target\n",
    "correlations = []\n",
    "for measure in centrality_measures:\n",
    "    corr = centrality_df[measure].corr(centrality_df['class'])\n",
    "    correlations.append((measure, corr))\n",
    "\n",
    "# Sort by absolute correlation\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display correlations\n",
    "print(\"Correlations with the target class:\")\n",
    "for measure, corr in correlations:\n",
    "    print(f\"{measure}: {corr:.4f}\")\n",
    "\n",
    "# Create a bar chart of correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "measures, corrs = zip(*correlations)\n",
    "plt.bar(measures, corrs, color=['blue' if c > 0 else 'red' for c in corrs])\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.title('Correlation of Centrality Measures with Fraud')\n",
    "plt.xlabel('Centrality Measure')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Coefficients\n",
    "\n",
    "Let's compute clustering coefficients, which measure the degree to which nodes in a graph tend to cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_coefficients(df_nodes, G):\n",
    "    \"\"\"\n",
    "    Compute clustering coefficients for nodes in an efficient way.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        Dataframe containing node data\n",
    "    G : networkx.Graph\n",
    "        NetworkX graph\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    clustering_features : numpy.ndarray\n",
    "        Array of clustering coefficients for each node\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing clustering coefficients...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to undirected for clustering coefficient calculation\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    # Initialize features\n",
    "    clustering_features = np.zeros((len(df_nodes), 1))\n",
    "    \n",
    "    # Calculate clustering coefficients for all nodes at once (more efficient)\n",
    "    clustering_dict = nx.clustering(G_undirected)\n",
    "    \n",
    "    # Fill feature array\n",
    "    for i, node_id in tqdm(enumerate(df_nodes['txId']), desc=\"Processing clustering\", total=len(df_nodes)):\n",
    "        clustering_features[i, 0] = clustering_dict.get(node_id, 0)\n",
    "    \n",
    "    logger.info(f\"Generated clustering features in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    return clustering_features\n",
    "\n",
    "# Compute clustering coefficients\n",
    "try:\n",
    "    clustering_features = compute_clustering_coefficients(df_nodes, G)\n",
    "    \n",
    "    # Add clustering coefficient to the DataFrame\n",
    "    centrality_df['clustering_coefficient'] = clustering_features\n",
    "    \n",
    "    # Display statistics of clustering coefficients\n",
    "    print(\"Clustering coefficient statistics:\")\n",
    "    print(centrality_df['clustering_coefficient'].describe())\n",
    "    \n",
    "    # Visualize the distribution of clustering coefficients by class\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter values for each class\n",
    "    fraud_values = centrality_df[centrality_df['class'] == 1]['clustering_coefficient']\n",
    "    legit_values = centrality_df[centrality_df['class'] == 0]['clustering_coefficient']\n",
    "    \n",
    "    # Plot distributions\n",
    "    sns.kdeplot(legit_values, color='blue', label='Legitimate', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(fraud_values, color='red', label='Fraudulent', fill=True, alpha=0.3)\n",
    "    \n",
    "    plt.title('Clustering Coefficient Distribution by Class')\n",
    "    plt.xlabel('Clustering Coefficient')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print the average clustering coefficient for each class\n",
    "    print(\"Average clustering coefficient by class:\")\n",
    "    print(centrality_df.groupby('class')['clustering_coefficient'].mean())\n",
    "    \n",
    "    # Calculate correlation with the target\n",
    "    clustering_corr = centrality_df['clustering_coefficient'].corr(centrality_df['class'])\n",
    "    print(f\"\\nCorrelation with the target class: {clustering_corr:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error computing clustering coefficients: {str(e)}. Skipping.\")\n",
    "    clustering_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features\n",
    "\n",
    "If the dataset contains time-related columns, we can extract temporal features that might be useful for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df_nodes):\n",
    "    \"\"\"\n",
    "    Extract temporal features if time-related columns are available.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_nodes : pandas.DataFrame\n",
    "        Dataframe containing node data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    temporal_features : numpy.ndarray or None\n",
    "        Array of temporal features, or None if no temporal data exists\n",
    "    \"\"\"\n",
    "    # Check if time-related columns exist\n",
    "    time_columns = [col for col in df_nodes.columns if 'time' in col.lower()]\n",
    "    \n",
    "    if not time_columns:\n",
    "        logger.info(\"No temporal features found in the dataset\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(f\"Extracting temporal features from columns: {time_columns}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract temporal features\n",
    "    temporal_features = df_nodes[time_columns].values\n",
    "    \n",
    "    logger.info(f\"Extracted temporal features in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    return temporal_features\n",
    "\n",
    "# Extract temporal features\n",
    "temporal_features = extract_temporal_features(df_nodes)\n",
    "\n",
    "if temporal_features is not None:\n",
    "    # Create a DataFrame with the temporal features\n",
    "    time_columns = [col for col in df_nodes.columns if 'time' in col.lower()]\n",
    "    temporal_df = pd.DataFrame(temporal_features, columns=time_columns)\n",
    "    \n",
    "    # Add transaction ID and class for reference\n",
    "    temporal_df['txId'] = df_nodes['txId'].values\n",
    "    temporal_df['class'] = df_nodes['class'].values\n",
    "    \n",
    "    # Display statistics of temporal features\n",
    "    print(\"Temporal features statistics:\")\n",
    "    print(temporal_df.describe())\n",
    "    \n",
    "    # Calculate correlations with the target\n",
    "    temporal_corrs = []\n",
    "    for col in time_columns:\n",
    "        corr = temporal_df[col].corr(temporal_df['class'])\n",
    "        temporal_corrs.append((col, corr))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    temporal_corrs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    # Display correlations\n",
    "    print(\"\\nCorrelations with the target class:\")\n",
    "    for col, corr in temporal_corrs:\n",
    "        print(f\"{col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Features\n",
    "\n",
    "Now, let's combine the original transaction features with the graph-based features we've computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(transaction_features, graph_features, clustering_features=None, temporal_features=None, normalize=True):\n",
    "    \"\"\"\n",
    "    Combine different feature sets and optionally normalize them.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transaction_features : numpy.ndarray\n",
    "        Original transaction features\n",
    "    graph_features : numpy.ndarray\n",
    "        Graph-based features\n",
    "    clustering_features : numpy.ndarray or None\n",
    "        Clustering coefficient features\n",
    "    temporal_features : numpy.ndarray or None\n",
    "        Temporal features, if available\n",
    "    normalize : bool\n",
    "        Whether to normalize the combined features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    combined_features : numpy.ndarray\n",
    "        Combined and normalized features\n",
    "    \"\"\"\n",
    "    logger.info(\"Combining feature sets...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    feature_list = [transaction_features, graph_features]\n",
    "    feature_types = [\"transaction\", \"graph\"]\n",
    "    feature_counts = [transaction_features.shape[1], graph_features.shape[1]]\n",
    "    \n",
    "    if clustering_features is not None:\n",
    "        feature_list.append(clustering_features)\n",
    "        feature_types.append(\"clustering\")\n",
    "        feature_counts.append(clustering_features.shape[1])\n",
    "    \n",
    "    if temporal_features is not None:\n",
    "        feature_list.append(temporal_features)\n",
    "        feature_types.append(\"temporal\")\n",
    "        feature_counts.append(temporal_features.shape[1])\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = np.hstack(feature_list)\n",
    "    \n",
    "    # Create a description of the feature combination\n",
    "    feature_desc = \", \".join([f\"{ftype} ({fcount})\" for ftype, fcount in zip(feature_types, feature_counts)])\n",
    "    logger.info(f\"Combined {feature_desc} features\")\n",
    "    \n",
    "    # Normalize features\n",
    "    if normalize:\n",
    "        logger.info(\"Normalizing combined features...\")\n",
    "        scaler = StandardScaler()\n",
    "        combined_features = scaler.fit_transform(combined_features)\n",
    "    \n",
    "    logger.info(f\"Final combined features shape: {combined_features.shape}, completed in {time.time()-start_time:.2f} seconds\")\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "# Get the original transaction features\n",
    "transaction_features = features\n",
    "\n",
    "# Combine all features\n",
    "combined_features = combine_features(\n",
    "    transaction_features=transaction_features,\n",
    "    graph_features=centrality_features,\n",
    "    clustering_features=clustering_features,\n",
    "    temporal_features=temporal_features\n",
    ")\n",
    "\n",
    "# Print the shape of the combined features\n",
    "print(f\"Original transaction features shape: {transaction_features.shape}\")\n",
    "print(f\"Graph centrality features shape: {centrality_features.shape}\")\n",
    "if clustering_features is not None:\n",
    "    print(f\"Clustering coefficient features shape: {clustering_features.shape}\")\n",
    "if temporal_features is not None:\n",
    "    print(f\"Temporal features shape: {temporal_features.shape}\")\n",
    "print(f\"Combined features shape: {combined_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's analyze the importance of the features for fraud detection using correlation and mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(features, labels, method='correlation'):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using the specified method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : numpy.ndarray\n",
    "        Feature matrix\n",
    "    labels : numpy.ndarray\n",
    "        Target labels\n",
    "    method : str\n",
    "        Method to calculate importance ('correlation', 'mutual_info', etc.)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_importance : numpy.ndarray\n",
    "        Array of importance scores for each feature\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating feature importance using {method}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'correlation':\n",
    "        # Calculate absolute correlation with target\n",
    "        correlations = np.zeros(features.shape[1])\n",
    "        \n",
    "        for i in tqdm(range(features.shape[1]), desc=\"Calculating correlations\"):\n",
    "            correlations[i] = abs(np.corrcoef(features[:, i], labels)[0, 1])\n",
    "        \n",
    "        logger.info(f\"Calculated feature importance in {time.time()-start_time:.2f} seconds\")\n",
    "        return correlations\n",
    "    \n",
    "    elif method == 'mutual_info':\n",
    "        # Calculate mutual information\n",
    "        logger.info(\"Computing mutual information...\")\n",
    "        importance = mutual_info_classif(features, labels)\n",
    "        \n",
    "        logger.info(f\"Calculated feature importance in {time.time()-start_time:.2f} seconds\")\n",
    "        return importance\n",
    "    \n",
    "    else:\n",
    "        logger.warning(f\"Unknown importance method: {method}, using correlation\")\n",
    "        return get_feature_importance(features, labels, method='correlation')\n",
    "\n",
    "# Calculate feature importance using correlation\n",
    "correlation_importance = get_feature_importance(combined_features, labels, method='correlation')\n",
    "\n",
    "# Calculate feature importance using mutual information\n",
    "mutual_info_importance = get_feature_importance(combined_features, labels, method='mutual_info')\n",
    "\n",
    "# Create a DataFrame to hold the importance scores\n",
    "feature_cols = [f\"feature_{i}\" for i in range(transaction_features.shape[1])]\n",
    "graph_cols = ['in_degree', 'out_degree', 'total_degree', 'pagerank', 'hub_score', 'authority_score']\n",
    "\n",
    "all_cols = feature_cols + graph_cols\n",
    "if clustering_features is not None:\n",
    "    all_cols.append('clustering_coefficient')\n",
    "if temporal_features is not None:\n",
    "    time_cols = [col for col in df_nodes.columns if 'time' in col.lower()]\n",
    "    all_cols.extend(time_cols)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': all_cols,\n",
    "    'correlation_importance': correlation_importance,\n",
    "    'mutual_info_importance': mutual_info_importance\n",
    "})\n",
    "\n",
    "# Sort by mutual information importance\n",
    "importance_df = importance_df.sort_values('mutual_info_importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the top 20 most important features\n",
    "print(\"Top 20 most important features (by mutual information):\")\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the top features by importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot top 15 features by correlation importance\n",
    "corr_top = importance_df.sort_values('correlation_importance', ascending=False).head(15)\n",
    "sns.barplot(x='correlation_importance', y='feature', data=corr_top, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Top 15 Features by Correlation Importance')\n",
    "axes[0].set_xlabel('Absolute Correlation')\n",
    "axes[0].set_ylabel('Feature')\n",
    "axes[0].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Plot top 15 features by mutual information importance\n",
    "mi_top = importance_df.sort_values('mutual_info_importance', ascending=False).head(15)\n",
    "sns.barplot(x='mutual_info_importance', y='feature', data=mi_top, ax=axes[1], palette='viridis')\n",
    "axes[1].set_title('Top 15 Features by Mutual Information')\n",
    "axes[1].set_xlabel('Mutual Information')\n",
    "axes[1].set_ylabel('Feature')\n",
    "axes[1].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if our graph-based features are among the top features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the graph features are among the top features\n",
    "graph_cols_set = set(graph_cols)\n",
    "if clustering_features is not None:\n",
    "    graph_cols_set.add('clustering_coefficient')\n",
    "\n",
    "# Get top 20 features\n",
    "top_features = importance_df.head(20)['feature'].tolist()\n",
    "top_graph_features = [f for f in top_features if f in graph_cols_set]\n",
    "\n",
    "print(f\"Number of graph-based features in the top 20: {len(top_graph_features)}\")\n",
    "print(\"Top graph-based features:\")\n",
    "for feature in top_graph_features:\n",
    "    rank = importance_df[importance_df['feature'] == feature].index[0] + 1\n",
    "    corr = importance_df[importance_df['feature'] == feature]['correlation_importance'].values[0]\n",
    "    mi = importance_df[importance_df['feature'] == feature]['mutual_info_importance'].values[0]\n",
    "    print(f\"  {rank}. {feature} - Correlation: {corr:.4f}, Mutual Info: {mi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Engineered Features\n",
    "\n",
    "Finally, let's save the engineered features for use in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined features\n",
    "output_dir = 'data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(output_dir, 'combined_features.npy'), combined_features)\n",
    "np.save(os.path.join(output_dir, 'feature_importance.npy'), mutual_info_importance)\n",
    "\n",
    "# Save feature names\n",
    "with open(os.path.join(output_dir, 'feature_names.txt'), 'w') as f:\n",
    "    for name in all_cols:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(\"Saved engineered features to data/processed/ directory:\")\n",
    "print(f\"  - combined_features.npy: Combined features matrix with shape {combined_features.shape}\")\n",
    "print(f\"  - feature_importance.npy: Feature importance scores with shape {mutual_info_importance.shape}\")\n",
    "print(f\"  - feature_names.txt: Names of all {len(all_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've conducted comprehensive feature engineering for the Bitcoin transaction fraud detection project. We've:\n",
    "\n",
    "1. Created a graph representation of the transaction network and analyzed its structure\n",
    "2. Computed centrality measures (in-degree, out-degree, PageRank, HITS) to identify important nodes\n",
    "3. Computed clustering coefficients to capture local network structure\n",
    "4. Extracted temporal features (if available)\n",
    "5. Combined all features and analyzed their importance for fraud detection\n",
    "6. Saved the engineered features for use in model training\n",
    "\n",
    "The graph-based features we've created provide valuable information about the structure and patterns of transactions, which can help improve fraud detection performance. In the next notebook, we'll implement and train Graph Neural Network models using these features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
